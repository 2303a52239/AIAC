{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzZN71+WJ1KynEqV4Qc79d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303a52239/AIAC/blob/main/12_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.  s.vyshnavi\n",
        "2.   2303A52239\n",
        "3. batch-35\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RRz9x5lIqvbq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "K5TzvbMDmNlf",
        "outputId": "6296c0ed-f661-4f17-a044-b8d787efe9f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Testing merge_sort ---\n",
            "Input: [], Expected: [], Got: []\n",
            "  Test Passed\n",
            "Input: [1], Expected: [1], Got: [1]\n",
            "  Test Passed\n",
            "Input: [5, 2, 8, 1, 9], Expected: [1, 2, 5, 8, 9], Got: [1, 2, 5, 8, 9]\n",
            "  Test Passed\n",
            "Input: [1, 2, 3, 4, 5], Expected: [1, 2, 3, 4, 5], Got: [1, 2, 3, 4, 5]\n",
            "  Test Passed\n",
            "Input: [5, 4, 3, 2, 1], Expected: [1, 2, 3, 4, 5], Got: [1, 2, 3, 4, 5]\n",
            "  Test Passed\n",
            "Input: [3, 1, 4, 1, 5, 9, 2, 6], Expected: [1, 1, 2, 3, 4, 5, 6, 9], Got: [1, 1, 2, 3, 4, 5, 6, 9]\n",
            "  Test Passed\n",
            "Input: [0, -1, 10, -5, 7], Expected: [-5, -1, 0, 7, 10], Got: [-5, -1, 0, 7, 10]\n",
            "  Test Passed\n",
            "All test cases passed for merge_sort!\n"
          ]
        }
      ],
      "source": [
        "def merge_sort(arr):\n",
        "    \"\"\"\n",
        "    Sorts a list in ascending order using the Merge Sort algorithm.\n",
        "\n",
        "    Args:\n",
        "        arr (list): The list to be sorted.\n",
        "\n",
        "    Returns:\n",
        "        list: The sorted list.\n",
        "\n",
        "    Time Complexity:\n",
        "        O(n log n) in all cases (best, average, worst).\n",
        "        This is because the array is divided into two halves at each step (log n divisions),\n",
        "        and merging takes O(n) time.\n",
        "\n",
        "    Space Complexity:\n",
        "        O(n) due to the temporary arrays created during the merging process.\n",
        "    \"\"\"\n",
        "    if len(arr) <= 1:\n",
        "        return arr\n",
        "\n",
        "    mid = len(arr) // 2\n",
        "    left_half = arr[:mid]\n",
        "    right_half = arr[mid:]\n",
        "\n",
        "    left_half = merge_sort(left_half)\n",
        "    right_half = merge_sort(right_half)\n",
        "\n",
        "    return merge(left_half, right_half)\n",
        "\n",
        "def merge(left, right):\n",
        "    merged = []\n",
        "    left_idx, right_idx = 0, 0\n",
        "\n",
        "    while left_idx < len(left) and right_idx < len(right):\n",
        "        if left[left_idx] < right[right_idx]:\n",
        "            merged.append(left[left_idx])\n",
        "            left_idx += 1\n",
        "        else:\n",
        "            merged.append(right[right_idx])\n",
        "            right_idx += 1\n",
        "\n",
        "    merged.extend(left[left_idx:])\n",
        "    merged.extend(right[right_idx:])\n",
        "    return merged\n",
        "\n",
        "# Test cases\n",
        "print(\"--- Testing merge_sort ---\")\n",
        "\n",
        "test_cases = [\n",
        "    ([], []),  # Empty list\n",
        "    ([1], [1]),  # Single element list\n",
        "    ([5, 2, 8, 1, 9], [1, 2, 5, 8, 9]),  # General case\n",
        "    ([1, 2, 3, 4, 5], [1, 2, 3, 4, 5]),  # Already sorted\n",
        "    ([5, 4, 3, 2, 1], [1, 2, 3, 4, 5]),  # Reverse sorted\n",
        "    ([3, 1, 4, 1, 5, 9, 2, 6], [1, 1, 2, 3, 4, 5, 6, 9]),  # List with duplicates\n",
        "    ([0, -1, 10, -5, 7], [-5, -1, 0, 7, 10]) # List with negative numbers\n",
        "]\n",
        "\n",
        "for input_arr, expected_output in test_cases:\n",
        "    result = merge_sort(input_arr)\n",
        "    print(f\"Input: {input_arr}, Expected: {expected_output}, Got: {result}\")\n",
        "    assert result == expected_output, f\"Test failed for input {input_arr}. Expected {expected_output}, got {result}\"\n",
        "    print(\"  Test Passed\")\n",
        "\n",
        "print(\"All test cases passed for merge_sort!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_search(arr, target):\n",
        "    \"\"\"\n",
        "    Performs a binary search on a sorted list to find the index of a target element.\n",
        "\n",
        "    Args:\n",
        "        arr (list): A sorted list of elements.\n",
        "        target: The element to search for.\n",
        "\n",
        "    Returns:\n",
        "        int: The index of the target if found, otherwise -1.\n",
        "\n",
        "    Time Complexity:\n",
        "        Best Case: O(1) - when the target is the middle element on the first comparison.\n",
        "        Average Case: O(log n) - the search space is halved in each step.\n",
        "        Worst Case: O(log n) - when the target is at an edge or not present, requiring log n comparisons.\n",
        "\n",
        "    Space Complexity:\n",
        "        O(1) - for iterative approach, as only a few variables are used.\n",
        "    \"\"\"\n",
        "    left, right = 0, len(arr) - 1\n",
        "\n",
        "    while left <= right:\n",
        "        mid = left + (right - left) // 2  # To prevent potential overflow compared to (left + right) // 2\n",
        "\n",
        "        if arr[mid] == target:\n",
        "            return mid\n",
        "        elif arr[mid] < target:\n",
        "            left = mid + 1\n",
        "        else:\n",
        "            right = mid - 1\n",
        "    return -1\n",
        "\n",
        "# Test cases\n",
        "print(\"--- Testing binary_search ---\")\n",
        "\n",
        "test_cases = [\n",
        "    ([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 5, 4),  # Target in middle\n",
        "    ([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 1, 0),  # Target at beginning\n",
        "    ([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 10, 9), # Target at end\n",
        "    ([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 11, -1), # Target not found (greater)\n",
        "    ([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 0, -1),  # Target not found (smaller)\n",
        "    ([], 5, -1),                             # Empty list\n",
        "    ([7], 7, 0),                             # Single element list (found)\n",
        "    ([7], 5, -1),                            # Single element list (not found)\n",
        "    ([1, 3, 5, 7, 9], 3, 1),                # Odd number of elements, target found\n",
        "    ([2, 4, 6, 8, 10], 7, -1),              # Odd number of elements, target not found\n",
        "    ([1, 2, 4, 5, 6, 7, 8, 9, 10], 3, -1) # Target not found between elements\n",
        "]\n",
        "\n",
        "for arr, target, expected_index in test_cases:\n",
        "    result = binary_search(arr, target)\n",
        "    print(f\"Searching for {target} in {arr}. Expected: {expected_index}, Got: {result}\")\n",
        "    assert result == expected_index, f\"Test failed for arr={arr}, target={target}. Expected {expected_index}, got {result}\"\n",
        "    print(\"  Test Passed\")\n",
        "\n",
        "print(\"All test cases passed for binary_search!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bJEt8s8Hm0ZA",
        "outputId": "326d3483-1b79-46c8-b2e9-23d3be1e4aa2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Testing binary_search ---\n",
            "Searching for 5 in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]. Expected: 4, Got: 4\n",
            "  Test Passed\n",
            "Searching for 1 in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]. Expected: 0, Got: 0\n",
            "  Test Passed\n",
            "Searching for 10 in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]. Expected: 9, Got: 9\n",
            "  Test Passed\n",
            "Searching for 11 in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]. Expected: -1, Got: -1\n",
            "  Test Passed\n",
            "Searching for 0 in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]. Expected: -1, Got: -1\n",
            "  Test Passed\n",
            "Searching for 5 in []. Expected: -1, Got: -1\n",
            "  Test Passed\n",
            "Searching for 7 in [7]. Expected: 0, Got: 0\n",
            "  Test Passed\n",
            "Searching for 5 in [7]. Expected: -1, Got: -1\n",
            "  Test Passed\n",
            "Searching for 3 in [1, 3, 5, 7, 9]. Expected: 1, Got: 1\n",
            "  Test Passed\n",
            "Searching for 7 in [2, 4, 6, 8, 10]. Expected: -1, Got: -1\n",
            "  Test Passed\n",
            "Searching for 3 in [1, 2, 4, 5, 6, 7, 8, 9, 10]. Expected: -1, Got: -1\n",
            "  Test Passed\n",
            "All test cases passed for binary_search!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ncd-UY8BnSll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9880bd71"
      },
      "source": [
        "# Task\n",
        "```python\n",
        "# Justification Table for Algorithms\n",
        "print(\"## Algorithm Justifications for Retail Inventory System\\n\")\n",
        "print(\"| Feature            | Algorithm Recommendation                               | Justification                                                                                                                                                                                                                                                                                                            |\n",
        "|--------------------|----------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| **Product Search** |                                        |                                                                                                                                                                                                                                                                                                                                                        |\n",
        "| By Product ID      | Hash Map (Dictionary in Python)        | - **Performance:** O(1) average time complexity for lookups, insertions, and deletions. Crucial for high-throughput systems where quick access by unique ID is paramount.<br>- **Dataset Size:** Scales extremely well with large datasets due to constant time operations.<br>- **Update Frequency:** Efficient for frequent updates (add/remove products) as hash map operations are fast.<br>- **Use Case:** Ideal for fetching product details rapidly when the exact ID is known (e.g., checkout systems, inventory management). |\n",
        "| By Product Name    | Binary Search on Sorted List           | - **Performance:** O(log n) time complexity for search on a sorted list. More efficient than linear search (O(n)) for large datasets.<br>- **Dataset Size:** Good for moderately large datasets. For extremely large, frequently searched datasets, a dedicated search index (like a trie or inverted index) might be considered, but binary search is a strong fundamental.<br>- **Update Frequency:** If the list is frequently updated (insertions/deletions), re-sorting or maintaining a sorted order can be O(n log n) or O(n), which might impact performance. For less frequent updates, it's suitable.<br>- **Alternative for frequent lookups:** A hash map of `name: product_object` would offer O(1) average lookup, but would consume more memory and require careful handling of duplicate names.<br>- **Use Case:** Useful for customer-facing searches where partial or exact name matches are needed, especially if the data is stable or sorted periodically. |\n",
        "| **Product Sorting**|                                        |                                                                                                                                                                                                                                                                                                                                                        |\n",
        "| By Price           | Merge Sort or Timsort (Python's default)| - **Performance:** O(n log n) time complexity in all cases (best, average, worst). This is optimal for comparison-based sorting algorithms.<br>- **Stability:** Merge Sort is a stable sorting algorithm, meaning that elements with equal sort keys maintain their relative order, which can be important for secondary sort criteria.<br>- **Dataset Size:** Highly efficient for large datasets.<br>- **Update Frequency:** Sorting is typically done less frequently than searches, or on subsets of data. If the entire inventory needs to be sorted frequently, the O(n log n) cost should be considered. Python's `sort()` (Timsort) is an optimized hybrid that performs very well in practice.<br>- **Use Case:** Generating reports (e.g., \"top 10 cheapest products\"), displaying products in e-commerce interfaces, or analyzing price distributions. |\n",
        "| By Quantity        | Merge Sort or Timsort (Python's default)| - **Performance:** O(n log n) time complexity. Similar to sorting by price, it offers guaranteed performance characteristics.<br>- **Stability:** Ensures consistent ordering for items with the same quantity.<br>- **Dataset Size:** Effective for sorting inventories of any practical size.<br>- **Update Frequency:** As with price, sorting is usually an on-demand operation. If quantity changes are very frequent, re-sorting the entire list might be expensive, but typically inventory analysis or display sorts are not real-time on the entire dataset.<br>- **Use Case:** Identifying low-stock items, managing warehouse logistics, or analyzing product availability. |\n",
        "\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47733361"
      },
      "source": [
        "## Justification Table for Algorithms\n",
        "\n",
        "### Subtask:\n",
        "Create a markdown table outlining the recommended algorithms for product search (by ID and by name) and sorting (by price and by quantity), along with justifications for each choice based on dataset size, update frequency, and performance requirements.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "155aa9c5"
      },
      "source": [
        "## Justification Table for Algorithms\n",
        "\n",
        "### Subtask:\n",
        "Create a markdown table outlining the recommended algorithms for product search (by ID and by name) and sorting (by price and by quantity), along with justifications for each choice based on dataset size, update frequency, and performance requirements.\n",
        "\n",
        "### Recommended Algorithms and Justifications\n",
        "\n",
        "| Feature              | Algorithm Recommendation | Justification (Dataset Size, Update Frequency, Performance)                                                                                                                                                                                                                                                                                                                      | Example Use Case                                                                                                    |\n",
        "| :------------------- | :----------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------ |\n",
        "| **Product Search**   |                          |                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                     |\n",
        "| By ID (Exact Match)  | Hash Map (Dictionary)    | **Dataset Size:** O(1) average time complexity for lookup, insertion, and deletion, making it highly efficient for any dataset size (small to very large) where direct access by ID is needed. <br/> **Update Frequency:** Efficient for frequent additions/deletions/updates of products as these operations also average O(1). <br/> **Performance:** Provides near-instantaneous retrieval of specific product details, crucial for real-time inventory lookups or order processing. | Quickly retrieve product details for a given SKU in a large catalog for a customer service agent.                 |\n",
        "| By Name (Partial/Fuzzy Match) | Trie (Prefix Tree) / Full-text Search Engine (e.g., Elasticsearch, Lucene) | **Dataset Size:** Trie is efficient for prefix searches on large datasets (O(L) where L is query length). Full-text search engines handle very large, unstructured text data. <br/> **Update Frequency:** Trie updates are O(L) for insertion/deletion. Full-text search engines are optimized for frequent indexing and searching on dynamic data. <br/> **Performance:** Trie offers fast auto-completion and suggestion features. Full-text search provides highly relevant results for complex queries, synonym matching, and fault tolerance (typos). | Customer typing product name and getting suggestions, or searching for products with complex search terms.        |\n",
        "| **Product Sorting**  |                          |                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                     |\n",
        "| By Price             | Merge Sort / Timsort (Python's default) | **Dataset Size:** O(N log N) time complexity, which is efficient and scales well for large datasets. It's a stable sort. <br/> **Update Frequency:** If data is frequently updated, the entire list needs to be re-sorted. For very high frequency, consider keeping a sorted data structure (like a balanced BST or heap) or incremental sorting if applicable. <br/> **Performance:** Guarantees O(N log N) worst-case performance, suitable for user-facing sorted lists where consistent speed is important. | Displaying products from lowest to highest price on an e-commerce category page.                                  |\n",
        "| By Quantity (Stock)  | Merge Sort / Timsort (Python's default) | **Dataset Size:** O(N log N) time complexity, effective for sorting inventories of varying sizes. <br/> **Update Frequency:** Similar to sorting by price; frequent updates necessitate re-sorting or using a dynamically sorted structure. <br/> **Performance:** Reliable performance for sorting product stock levels, enabling efficient identification of low-stock items or best-sellers. | Identifying top-selling products by current stock level or listing products that are nearly out of stock.        |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb18937c"
      },
      "source": [
        "## Justification Table for Algorithms\n",
        "\n",
        "### Subtask:\n",
        "Create a markdown table outlining the recommended algorithms for product search (by ID and by name) and sorting (by price and by quantity), along with justifications for each choice based on dataset size, update frequency, and performance requirements.\n",
        "\n",
        "### Recommended Algorithms and Justifications\n",
        "\n",
        "| Feature              | Algorithm Recommendation | Justification (Dataset Size, Update Frequency, Performance)                                                                                                                                                                                                                                                                                                                      | Example Use Case                                                                                                    |\n",
        "| :------------------- | :----------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------ |\n",
        "| **Product Search**   |                          |                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                     |\n",
        "| By ID (Exact Match)  | Hash Map (Dictionary)    | **Dataset Size:** O(1) average time complexity for lookup, insertion, and deletion, making it highly efficient for any dataset size (small to very large) where direct access by ID is needed. <br/> **Update Frequency:** Efficient for frequent additions/deletions/updates of products as these operations also average O(1). <br/> **Performance:** Provides near-instantaneous retrieval of specific product details, crucial for real-time inventory lookups or order processing. | Quickly retrieve product details for a given SKU in a large catalog for a customer service agent.                 |\n",
        "| By Name (Partial/Fuzzy Match) | Trie (Prefix Tree) / Full-text Search Engine (e.g., Elasticsearch, Lucene) | **Dataset Size:** Trie is efficient for prefix searches on large datasets (O(L) where L is query length). Full-text search engines handle very large, unstructured text data. <br/> **Update Frequency:** Trie updates are O(L) for insertion/deletion. Full-text search engines are optimized for frequent indexing and searching on dynamic data. <br/> **Performance:** Trie offers fast auto-completion and suggestion features. Full-text search provides highly relevant results for complex queries, synonym matching, and fault tolerance (typos). | Customer typing product name and getting suggestions, or searching for products with complex search terms.        |\n",
        "| **Product Sorting**  |                          |                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                     |\n",
        "| By Price             | Merge Sort / Timsort (Python's default) | **Dataset Size:** O(N log N) time complexity, which is efficient and scales well for large datasets. It's a stable sort. <br/> **Update Frequency:** If data is frequently updated, the entire list needs to be re-sorted. For very high frequency, consider keeping a sorted data structure (like a balanced BST or heap) or incremental sorting if applicable. <br/> **Performance:** Guarantees O(N log N) worst-case performance, suitable for user-facing sorted lists where consistent speed is important. | Displaying products from lowest to highest price on an e-commerce category page.                                  |\n",
        "| By Quantity (Stock)  | Merge Sort / Timsort (Python's default) | **Dataset Size:** O(N log N) time complexity, effective for sorting inventories of varying sizes. <br/> **Update Frequency:** Similar to sorting by price; frequent updates necessitate re-sorting or using a dynamically sorted structure. <br/> **Performance:** Reliable performance for sorting product stock levels, enabling efficient identification of low-stock items or best-sellers. | Identifying top-selling products by current stock level or listing products that are nearly out of stock.        |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ef0e1d8"
      },
      "source": [
        "## Define Sample Product Data\n",
        "\n",
        "### Subtask:\n",
        "Create a sample list of product dictionaries, each containing 'product_id', 'name', 'price', and 'quantity' attributes, to serve as the inventory for demonstrating the search and sort functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42a767f5"
      },
      "source": [
        "**Reasoning**:\n",
        "To fulfill the subtask, I need to create a Python list of product dictionaries as specified in the instructions. This `code_block` will define the `products` list with sample data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "54db76bf",
        "outputId": "0c3f4774-b244-43ce-bc3b-d68ce77a957a"
      },
      "source": [
        "products = [\n",
        "    {'product_id': 'P001', 'name': 'Laptop', 'price': 1200.00, 'quantity': 10},\n",
        "    {'product_id': 'P002', 'name': 'Mouse', 'price': 25.50, 'quantity': 50},\n",
        "    {'product_id': 'P003', 'name': 'Keyboard', 'price': 75.00, 'quantity': 30},\n",
        "    {'product_id': 'P004', 'name': 'Monitor', 'price': 300.00, 'quantity': 5},\n",
        "    {'product_id': 'P005', 'name': 'Webcam', 'price': 50.00, 'quantity': 20},\n",
        "    {'product_id': 'P006', 'name': 'Headphones', 'price': 150.00, 'quantity': 15},\n",
        "    {'product_id': 'P007', 'name': 'USB Drive', 'price': 15.00, 'quantity': 100}\n",
        "]\n",
        "\n",
        "print(\"Sample product data created:\")\n",
        "for product in products:\n",
        "    print(product)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample product data created:\n",
            "{'product_id': 'P001', 'name': 'Laptop', 'price': 1200.0, 'quantity': 10}\n",
            "{'product_id': 'P002', 'name': 'Mouse', 'price': 25.5, 'quantity': 50}\n",
            "{'product_id': 'P003', 'name': 'Keyboard', 'price': 75.0, 'quantity': 30}\n",
            "{'product_id': 'P004', 'name': 'Monitor', 'price': 300.0, 'quantity': 5}\n",
            "{'product_id': 'P005', 'name': 'Webcam', 'price': 50.0, 'quantity': 20}\n",
            "{'product_id': 'P006', 'name': 'Headphones', 'price': 150.0, 'quantity': 15}\n",
            "{'product_id': 'P007', 'name': 'USB Drive', 'price': 15.0, 'quantity': 100}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "527f3a2e"
      },
      "source": [
        "## Implement Efficient Search by Product ID\n",
        "\n",
        "### Subtask:\n",
        "Implement a Python function that efficiently searches for a product by its unique 'product_id'. This function will use a dictionary (hash map) for O(1) average time complexity lookups, demonstrating the most efficient approach for direct ID access.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "039d7f7e"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires implementing a Python function `search_product_by_id` that uses a hash map for efficient lookups. This code block will define the function as specified, including the hash map creation, lookup logic, and test cases.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9f441aef",
        "outputId": "d04ce15c-4a5d-45da-9e8e-3eda70ffec63"
      },
      "source": [
        "def search_product_by_id(products_list, product_id_to_find):\n",
        "    \"\"\"\n",
        "    Searches for a product by its unique product_id using a hash map (dictionary).\n",
        "\n",
        "    Args:\n",
        "        products_list (list): A list of product dictionaries.\n",
        "        product_id_to_find (str): The product_id to search for.\n",
        "\n",
        "    Returns:\n",
        "        dict or None: The product dictionary if found, otherwise None.\n",
        "    \"\"\"\n",
        "    # Create a hash map (dictionary) for O(1) average time complexity lookups\n",
        "    product_hash_map = {}\n",
        "    for product in products_list:\n",
        "        product_hash_map[product['product_id']] = product\n",
        "\n",
        "    # Look up the product_id in the hash map\n",
        "    return product_hash_map.get(product_id_to_find)\n",
        "\n",
        "# Test cases\n",
        "print(\"--- Testing search_product_by_id ---\")\n",
        "\n",
        "# Test with an existing product_id\n",
        "existing_id = 'P001'\n",
        "found_product = search_product_by_id(products, existing_id)\n",
        "print(f\"Searching for Product ID '{existing_id}': {found_product}\")\n",
        "assert found_product is not None and found_product['product_id'] == existing_id, f\"Test failed for existing ID {existing_id}\"\n",
        "print(f\"  Test Passed for existing ID: {existing_id}\")\n",
        "\n",
        "# Test with another existing product_id\n",
        "existing_id_2 = 'P005'\n",
        "found_product_2 = search_product_by_id(products, existing_id_2)\n",
        "print(f\"Searching for Product ID '{existing_id_2}': {found_product_2}\")\n",
        "assert found_product_2 is not None and found_product_2['product_id'] == existing_id_2, f\"Test failed for existing ID {existing_id_2}\"\n",
        "print(f\"  Test Passed for existing ID: {existing_id_2}\")\n",
        "\n",
        "# Test with a non-existent product_id\n",
        "non_existent_id = 'P999'\n",
        "not_found_product = search_product_by_id(products, non_existent_id)\n",
        "print(f\"Searching for Product ID '{non_existent_id}': {not_found_product}\")\n",
        "assert not_found_product is None, f\"Test failed for non-existent ID {non_existent_id}\"\n",
        "print(f\"  Test Passed for non-existent ID: {non_existent_id}\")\n",
        "\n",
        "print(\"All test cases passed for search_product_by_id!\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Testing search_product_by_id ---\n",
            "Searching for Product ID 'P001': {'product_id': 'P001', 'name': 'Laptop', 'price': 1200.0, 'quantity': 10}\n",
            "  Test Passed for existing ID: P001\n",
            "Searching for Product ID 'P005': {'product_id': 'P005', 'name': 'Webcam', 'price': 50.0, 'quantity': 20}\n",
            "  Test Passed for existing ID: P005\n",
            "Searching for Product ID 'P999': None\n",
            "  Test Passed for non-existent ID: P999\n",
            "All test cases passed for search_product_by_id!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "867b5f4d"
      },
      "source": [
        "## Implement Efficient Search by Product Name\n",
        "\n",
        "### Subtask:\n",
        "Implement a Python function that searches for a product by its 'name'. This will involve using the existing `binary_search` function on a list sorted by product name.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "749d73e6"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires implementing a Python function `search_product_by_name` that leverages the existing `binary_search` function. This code block will define the function as specified, including sorting the products by name, extracting names for binary search, and handling results, along with comprehensive test cases.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "df9cfd7e",
        "outputId": "d5e3ba9d-0030-4d28-c954-bc3c6f36fa7f"
      },
      "source": [
        "def search_product_by_name(products_list, target_name):\n",
        "    \"\"\"\n",
        "    Searches for a product by its name using binary search on a sorted list.\n",
        "\n",
        "    Args:\n",
        "        products_list (list): A list of product dictionaries.\n",
        "        target_name (str): The product name to search for.\n",
        "\n",
        "    Returns:\n",
        "        dict or None: The product dictionary if found, otherwise None.\n",
        "    \"\"\"\n",
        "    # 1. Sort the products list by 'name' attribute\n",
        "    # Convert names to lowercase for case-insensitive search if desired, but for now, exact match.\n",
        "    sorted_products = sorted(products_list, key=lambda x: x['name'])\n",
        "\n",
        "    # 2. Create a separate list containing only the 'name' values from the sorted product list\n",
        "    product_names = [product['name'] for product in sorted_products]\n",
        "\n",
        "    # 3. Utilize the previously defined binary_search function\n",
        "    index = binary_search(product_names, target_name)\n",
        "\n",
        "    # 4. If the binary_search returns a valid index, retrieve the corresponding product dictionary\n",
        "    if index != -1:\n",
        "        return sorted_products[index]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Test cases\n",
        "print(\"--- Testing search_product_by_name ---\")\n",
        "\n",
        "test_cases_name = [\n",
        "    ('Laptop', {'product_id': 'P001', 'name': 'Laptop', 'price': 1200.00, 'quantity': 10}), # Exact match\n",
        "    ('Webcam', {'product_id': 'P005', 'name': 'Webcam', 'price': 50.00, 'quantity': 20}),   # Another exact match\n",
        "    ('Monitor', {'product_id': 'P004', 'name': 'Monitor', 'price': 300.00, 'quantity': 5}), # Another exact match\n",
        "    ('Tablet', None), # Non-existent name\n",
        "    ('laptop', None), # Case-sensitive (should not find if 'Laptop' is the actual name)\n",
        "    ('Mouse', {'product_id': 'P002', 'name': 'Mouse', 'price': 25.50, 'quantity': 50}),\n",
        "    ('USB Drive', {'product_id': 'P007', 'name': 'USB Drive', 'price': 15.00, 'quantity': 100})\n",
        "]\n",
        "\n",
        "for target_name, expected_product in test_cases_name:\n",
        "    result = search_product_by_name(products, target_name)\n",
        "    print(f\"Searching for name '{target_name}'. Expected: {expected_product}, Got: {result}\")\n",
        "    assert result == expected_product, f\"Test failed for name '{target_name}'. Expected {expected_product}, got {result}\"\n",
        "    print(f\"  Test Passed for name: {target_name}\")\n",
        "\n",
        "print(\"All test cases passed for search_product_by_name!\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Testing search_product_by_name ---\n",
            "Searching for name 'Laptop'. Expected: {'product_id': 'P001', 'name': 'Laptop', 'price': 1200.0, 'quantity': 10}, Got: {'product_id': 'P001', 'name': 'Laptop', 'price': 1200.0, 'quantity': 10}\n",
            "  Test Passed for name: Laptop\n",
            "Searching for name 'Webcam'. Expected: {'product_id': 'P005', 'name': 'Webcam', 'price': 50.0, 'quantity': 20}, Got: {'product_id': 'P005', 'name': 'Webcam', 'price': 50.0, 'quantity': 20}\n",
            "  Test Passed for name: Webcam\n",
            "Searching for name 'Monitor'. Expected: {'product_id': 'P004', 'name': 'Monitor', 'price': 300.0, 'quantity': 5}, Got: {'product_id': 'P004', 'name': 'Monitor', 'price': 300.0, 'quantity': 5}\n",
            "  Test Passed for name: Monitor\n",
            "Searching for name 'Tablet'. Expected: None, Got: None\n",
            "  Test Passed for name: Tablet\n",
            "Searching for name 'laptop'. Expected: None, Got: None\n",
            "  Test Passed for name: laptop\n",
            "Searching for name 'Mouse'. Expected: {'product_id': 'P002', 'name': 'Mouse', 'price': 25.5, 'quantity': 50}, Got: {'product_id': 'P002', 'name': 'Mouse', 'price': 25.5, 'quantity': 50}\n",
            "  Test Passed for name: Mouse\n",
            "Searching for name 'USB Drive'. Expected: {'product_id': 'P007', 'name': 'USB Drive', 'price': 15.0, 'quantity': 100}, Got: {'product_id': 'P007', 'name': 'USB Drive', 'price': 15.0, 'quantity': 100}\n",
            "  Test Passed for name: USB Drive\n",
            "All test cases passed for search_product_by_name!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fec4d711"
      },
      "source": [
        "## Demonstrate Product Sorting by Price and Quantity\n",
        "\n",
        "### Subtask:\n",
        "Demonstrate the usage of the existing `merge_sort` function to sort the sample product data. This will involve sorting the list of product dictionaries first by their 'price' attribute and then by their 'quantity' attribute, showcasing the algorithm's application.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04d079d4"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires demonstrating the `merge_sort` function for sorting a list of product dictionaries by 'price' and 'quantity'. Since the existing `merge_sort` function compares elements directly and dictionaries are not directly comparable using `<`, I will create a wrapper function `sort_products_by_key`. This function will transform the list of product dictionaries into a list of `(key_value, product_dictionary)` tuples, which `merge_sort` can sort based on the `key_value` (due to Python's tuple comparison). After sorting, the original product dictionaries will be extracted. This approach allows using the `merge_sort` function as is, without modification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "af41149c",
        "outputId": "5d7ba1e2-87dc-43b9-82a4-e10c5476cf78"
      },
      "source": [
        "def sort_products_by_key(products_list, key_name):\n",
        "    \"\"\"\n",
        "    Sorts a list of product dictionaries using the existing merge_sort function\n",
        "    based on a specified key. It works by creating temporary tuples (key_value, product_dict)\n",
        "    and then extracting the sorted product dictionaries.\n",
        "\n",
        "    Args:\n",
        "        products_list (list): A list of product dictionaries.\n",
        "        key_name (str): The key in the dictionary to sort by (e.g., 'price', 'quantity').\n",
        "\n",
        "    Returns:\n",
        "        list: A new list of product dictionaries, sorted by the specified key.\n",
        "    \"\"\"\n",
        "    # Create a temporary list of tuples (key_value, product_dictionary)\n",
        "    # The existing merge_sort can sort these tuples based on their first element (the key_value).\n",
        "    items_to_sort = []\n",
        "    for product in products_list:\n",
        "        items_to_sort.append((product[key_name], product))\n",
        "\n",
        "    # Use the existing merge_sort function to sort these tuples\n",
        "    sorted_items = merge_sort(items_to_sort)\n",
        "\n",
        "    # Extract the original product dictionaries from the sorted tuples\n",
        "    sorted_products = [item[1] for item in sorted_items]\n",
        "    return sorted_products\n",
        "\n",
        "# Test cases for sorting\n",
        "print(\"--- Demonstrating Product Sorting by Price and Quantity ---\")\n",
        "\n",
        "# 1. Sort by price\n",
        "print(\"\\nSorting products by 'price':\")\n",
        "sorted_by_price = sort_products_by_key(products, 'price')\n",
        "for product in sorted_by_price:\n",
        "    print(product)\n",
        "\n",
        "# Verify sorting by price\n",
        "assert sorted_by_price[0]['price'] == 15.00, f\"Expected cheapest product price 15.00, got {sorted_by_price[0]['price']}\"\n",
        "assert sorted_by_price[-1]['price'] == 1200.00, f\"Expected most expensive product price 1200.00, got {sorted_by_price[-1]['price']}\"\n",
        "print(\"  Verification by price passed (first and last elements).\")\n",
        "\n",
        "# 2. Sort by quantity (using a fresh copy of products to ensure original order for sorting)\n",
        "print(\"\\nSorting products by 'quantity':\")\n",
        "# It's good practice to sort a copy if you need the original list untouched elsewhere\n",
        "sorted_by_quantity = sort_products_by_key(products.copy(), 'quantity')\n",
        "for product in sorted_by_quantity:\n",
        "    print(product)\n",
        "\n",
        "# Verify sorting by quantity\n",
        "assert sorted_by_quantity[0]['quantity'] == 5, f\"Expected lowest quantity 5, got {sorted_by_quantity[0]['quantity']}\"\n",
        "assert sorted_by_quantity[-1]['quantity'] == 100, f\"Expected highest quantity 100, got {sorted_by_quantity[-1]['quantity']}\"\n",
        "print(\"  Verification by quantity passed (first and last elements).\")\n",
        "\n",
        "print(\"\\nAll product sorting demonstrations and verifications complete!\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Demonstrating Product Sorting by Price and Quantity ---\n",
            "\n",
            "Sorting products by 'price':\n",
            "{'product_id': 'P007', 'name': 'USB Drive', 'price': 15.0, 'quantity': 100}\n",
            "{'product_id': 'P002', 'name': 'Mouse', 'price': 25.5, 'quantity': 50}\n",
            "{'product_id': 'P005', 'name': 'Webcam', 'price': 50.0, 'quantity': 20}\n",
            "{'product_id': 'P003', 'name': 'Keyboard', 'price': 75.0, 'quantity': 30}\n",
            "{'product_id': 'P006', 'name': 'Headphones', 'price': 150.0, 'quantity': 15}\n",
            "{'product_id': 'P004', 'name': 'Monitor', 'price': 300.0, 'quantity': 5}\n",
            "{'product_id': 'P001', 'name': 'Laptop', 'price': 1200.0, 'quantity': 10}\n",
            "  Verification by price passed (first and last elements).\n",
            "\n",
            "Sorting products by 'quantity':\n",
            "{'product_id': 'P004', 'name': 'Monitor', 'price': 300.0, 'quantity': 5}\n",
            "{'product_id': 'P001', 'name': 'Laptop', 'price': 1200.0, 'quantity': 10}\n",
            "{'product_id': 'P006', 'name': 'Headphones', 'price': 150.0, 'quantity': 15}\n",
            "{'product_id': 'P005', 'name': 'Webcam', 'price': 50.0, 'quantity': 20}\n",
            "{'product_id': 'P003', 'name': 'Keyboard', 'price': 75.0, 'quantity': 30}\n",
            "{'product_id': 'P002', 'name': 'Mouse', 'price': 25.5, 'quantity': 50}\n",
            "{'product_id': 'P007', 'name': 'USB Drive', 'price': 15.0, 'quantity': 100}\n",
            "  Verification by quantity passed (first and last elements).\n",
            "\n",
            "All product sorting demonstrations and verifications complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53755112"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Provide a summary of the recommended algorithms, their implementations, and how they address the specific requirements of the retail inventory system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fad53c4"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "The data analysis task involved recommending and implementing efficient algorithms for common retail inventory system operations: product search (by ID and by name) and product sorting (by price and by quantity).\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Product Search by ID:**\n",
        "    *   **Algorithm Recommended & Implemented:** Hash Map (Python Dictionary).\n",
        "    *   **Performance:** Achieves O(1) average time complexity for lookups, insertions, and deletions.\n",
        "    *   **Justification:** Ideal for high-throughput systems requiring near-instantaneous retrieval of product details by unique identifiers, regardless of dataset size or update frequency.\n",
        "*   **Product Search by Name:**\n",
        "    *   **Algorithm Recommended:** Trie (Prefix Tree) or Full-text Search Engine for partial/fuzzy matches.\n",
        "    *   **Algorithm Implemented:** Binary Search on a list sorted by name for exact matches.\n",
        "    *   **Performance:** Binary Search offers O(log n) time complexity for search.\n",
        "    *   **Justification:** Binary Search is efficient for exact name matches on sorted datasets. For more complex, customer-facing search needs (like auto-completion or typo tolerance), Tries or full-text search engines provide superior functionality. The current implementation is case-sensitive.\n",
        "*   **Product Sorting by Price and Quantity:**\n",
        "    *   **Algorithm Recommended & Implemented:** Merge Sort / Timsort (Python's default sort).\n",
        "    *   **Performance:** Guarantees O(N log N) time complexity in all cases.\n",
        "    *   **Justification:** Optimal for comparison-based sorting, stable, and scales efficiently for large datasets. It's suitable for generating reports (e.g., \"top 10 cheapest products\") or displaying products in sorted order, with the understanding that frequent data updates would necessitate re-sorting the list.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The current algorithmic choices provide a robust and efficient foundation for core inventory operations.\n",
        "*   To enhance user experience for product searches, especially for customer-facing interfaces, consider integrating a more advanced search solution like a Trie for auto-completion or a full-text search engine (e.g., Elasticsearch) for fuzzy matching and more complex queries.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3qWaWnkmoBmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ab9499d"
      },
      "source": [
        "# Task\n",
        "The task is to recommend and implement efficient data structures and algorithms for a retail inventory system, covering product search by ID and name, and product sorting by price and quantity, along with justifications and a summary of the approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43e41462"
      },
      "source": [
        "## Justification Table for Algorithms (Hospital Scenario)\n",
        "\n",
        "### Subtask:\n",
        "Create a markdown table outlining the recommended algorithms for patient record search (by ID and by name) and sorting (by severity level and by bill amount), along with justifications for each choice based on dataset size, update frequency, and performance requirements.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd668477"
      },
      "source": [
        "## Justification Table for Algorithms (Hospital Scenario)\n",
        "\n",
        "### Subtask:\n",
        "Create a markdown table outlining the recommended algorithms for patient record search (by ID and by name) and sorting (by severity level and by bill amount), along with justifications for each choice based on dataset size, update frequency, and performance requirements.\n",
        "\n",
        "### Recommended Algorithms and Justifications\n",
        "\n",
        "| Feature              | Algorithm Recommendation | Justification (Dataset Size, Update Frequency, Performance)                                                                                                                                                                                                                                                                                                                      | Example Use Case                                                                                                    |\n",
        "| :------------------- | :----------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------ |\n",
        "| **Patient Search**   |                          |                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                     |\n",
        "| By Patient ID (Exact Match)  | Hash Map (Dictionary)    | **Dataset Size:** O(1) average time complexity for lookup, insertion, and deletion, making it highly efficient for any dataset size (small to very large) where direct access by ID is needed. <br/> **Update Frequency:** Efficient for frequent additions/deletions/updates of patient records as these operations also average O(1). <br/> **Performance:** Provides near-instantaneous retrieval of specific patient details, crucial for real-time access by medical staff during emergencies. | Quickly retrieve a patient's full medical history using their unique patient ID at the admissions desk or ER. |\n",
        "| By Patient Name (Partial/Fuzzy Match) | Trie (Prefix Tree) / Full-text Search Engine (e.g., Elasticsearch, Lucene) | **Dataset Size:** Trie is efficient for prefix searches on large datasets (O(L) where L is query length). Full-text search engines handle very large, unstructured text data. <br/> **Update Frequency:** Trie updates are O(L) for insertion/deletion. Full-text search engines are optimized for frequent indexing and searching on dynamic data. <br/> **Performance:** Trie offers fast auto-completion for patient names. Full-text search provides highly relevant results for complex queries, synonym matching, and fault tolerance (typos) on patient demographics. | A doctor searching for a patient by a partial name input or to find all patients with a specific condition mentioned in their notes.        |\n",
        "| **Patient Sorting**  |                          |                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                     |\n",
        "| By Severity Level    | Merge Sort / Timsort (Python's default) | **Dataset Size:** O(N log N) time complexity, which is efficient and scales well for large datasets. It's a stable sort. <br/> **Update Frequency:** Severity levels can change, requiring re-sorting for up-to-date views. For very high frequency, consider keeping a dynamically sorted structure. <br/> **Performance:** Guarantees O(N log N) worst-case performance, suitable for consistently prioritizing patients in queues or displays. | Ordering patients in an emergency room queue from most severe to least severe. |\n",
        "| By Bill Amount       | Merge Sort / Timsort (Python's default) | **Dataset Size:** O(N log N) time complexity, effective for sorting financial records of varying sizes. <br/> **Update Frequency:** Similar to severity; bill amounts can change frequently with new services. <br/> **Performance:** Reliable performance for sorting financial data, enabling efficient analysis of outstanding bills or revenue generation. | Generating a report of patient bills from highest to lowest amount for accounting purposes or insurance claims.        |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "487c4909"
      },
      "source": [
        "## Justification Table for Algorithms (Hospital Scenario)\n",
        "\n",
        "### Subtask:\n",
        "Create a markdown table outlining the recommended algorithms for patient record search (by ID and by name) and sorting (by severity level and by bill amount), along with justifications for each choice based on dataset size, update frequency, and performance requirements.\n",
        "\n",
        "### Recommended Algorithms and Justifications\n",
        "\n",
        "| Feature              | Algorithm Recommendation | Justification (Dataset Size, Update Frequency, Performance)                                                                                                                                                                                                                                                                                                                      | Example Use Case                                                                                                    |\n",
        "| :------------------- | :----------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------ |\n",
        "| **Patient Search**   |                          |                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                     |\n",
        "| By Patient ID (Exact Match)  | Hash Map (Dictionary)    | **Dataset Size:** O(1) average time complexity for lookup, insertion, and deletion, making it highly efficient for any dataset size (small to very large) where direct access by ID is needed. <br/> **Update Frequency:** Efficient for frequent additions/deletions/updates of patient records as these operations also average O(1). <br/> **Performance:** Provides near-instantaneous retrieval of specific patient details, crucial for real-time access by medical staff during emergencies. | Quickly retrieve a patient's full medical history using their unique patient ID at the admissions desk or ER. |\n",
        "| By Patient Name (Partial/Fuzzy Match) | Trie (Prefix Tree) / Full-text Search Engine (e.g., Elasticsearch, Lucene) | **Dataset Size:** Trie is efficient for prefix searches on large datasets (O(L) where L is query length). Full-text search engines handle very large, unstructured text data. <br/> **Update Frequency:** Trie updates are O(L) for insertion/deletion. Full-text search engines are optimized for frequent indexing and searching on dynamic data. <br/> **Performance:** Trie offers fast auto-completion for patient names. Full-text search provides highly relevant results for complex queries, synonym matching, and fault tolerance (typos) on patient demographics. | A doctor searching for a patient by a partial name input or to find all patients with a specific condition mentioned in their notes.        |\n",
        "| **Patient Sorting**  |                          |                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                     |\n",
        "| By Severity Level    | Merge Sort / Timsort (Python's default) | **Dataset Size:** O(N log N) time complexity, which is efficient and scales well for large datasets. It's a stable sort. <br/> **Update Frequency:** Severity levels can change, requiring re-sorting for up-to-date views. For very high frequency, consider keeping a dynamically sorted structure. <br/> **Performance:** Guarantees O(N log N) worst-case performance, suitable for consistently prioritizing patients in queues or displays. | Ordering patients in an emergency room queue from most severe to least severe. |\n",
        "| By Bill Amount       | Merge Sort / Timsort (Python's default) | **Dataset Size:** O(N log N) time complexity, effective for sorting financial records of varying sizes. <br/> **Update Frequency:** Similar to severity; bill amounts can change frequently with new services. <br/> **Performance:** Reliable performance for sorting financial data, enabling efficient analysis of outstanding bills or revenue generation. | Generating a report of patient bills from highest to lowest amount for accounting purposes or insurance claims.        |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d81b2fd7"
      },
      "source": [
        "## Justification Table for Algorithms (Hospital Scenario)\n",
        "\n",
        "### Subtask:\n",
        "Create a markdown table outlining the recommended algorithms for patient record search (by ID and by name) and sorting (by severity level and by bill amount), along with justifications for each choice based on dataset size, update frequency, and performance requirements.\n",
        "\n",
        "### Recommended Algorithms and Justifications\n",
        "\n",
        "| Feature              | Algorithm Recommendation | Justification (Dataset Size, Update Frequency, Performance)                                                                                                                                                                                                                                                                                                                      | Example Use Case                                                                                                    |\n",
        "| :------------------- | :----------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------ |\n",
        "| **Patient Search**   |                          |                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                     |\n",
        "| By Patient ID (Exact Match)  | Hash Map (Dictionary)    | **Dataset Size:** O(1) average time complexity for lookup, insertion, and deletion, making it highly efficient for any dataset size (small to very large) where direct access by ID is needed. <br/> **Update Frequency:** Efficient for frequent additions/deletions/updates of patient records as these operations also average O(1). <br/> **Performance:** Provides near-instantaneous retrieval of specific patient details, crucial for real-time access by medical staff during emergencies. | Quickly retrieve a patient's full medical history using their unique patient ID at the admissions desk or ER. |\n",
        "| By Patient Name (Partial/Fuzzy Match) | Trie (Prefix Tree) / Full-text Search Engine (e.g., Elasticsearch, Lucene) | **Dataset Size:** Trie is efficient for prefix searches on large datasets (O(L) where L is query length). Full-text search engines handle very large, unstructured text data. <br/> **Update Frequency:** Trie updates are O(L) for insertion/deletion. Full-text search engines are optimized for frequent indexing and searching on dynamic data. <br/> **Performance:** Trie offers fast auto-completion for patient names. Full-text search provides highly relevant results for complex queries, synonym matching, and fault tolerance (typos) on patient demographics. | A doctor searching for a patient by a partial name input or to find all patients with a specific condition mentioned in their notes.        |\n",
        "| **Patient Sorting**  |                          |                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                     |\n",
        "| By Severity Level    | Merge Sort / Timsort (Python's default) | **Dataset Size:** O(N log N) time complexity, which is efficient and scales well for large datasets. It's a stable sort. <br/> **Update Frequency:** Severity levels can change, requiring re-sorting for up-to-date views. For very high frequency, consider keeping a dynamically sorted structure. <br/> **Performance:** Guarantees O(N log N) worst-case performance, suitable for consistently prioritizing patients in queues or displays. | Ordering patients in an emergency room queue from most severe to least severe. |\n",
        "| By Bill Amount       | Merge Sort / Timsort (Python's default) | **Dataset Size:** O(N log N) time complexity, effective for sorting financial records of varying sizes. <br/> **Update Frequency:** Similar to severity; bill amounts can change frequently with new services. <br/> **Performance:** Reliable performance for sorting financial data, enabling efficient analysis of outstanding bills or revenue generation. | Generating a report of patient bills from highest to lowest amount for accounting purposes or insurance claims.        |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f2fab5c"
      },
      "source": [
        "## Justification Table for Algorithms (Hospital Scenario)\n",
        "\n",
        "### Subtask:\n",
        "Create a markdown table outlining the recommended algorithms for patient record search (by ID and by name) and sorting (by severity level and by bill amount), along with justifications for each choice based on dataset size, update frequency, and performance requirements.\n",
        "\n",
        "### Recommended Algorithms and Justifications\n",
        "\n",
        "| Feature              | Algorithm Recommendation | Justification (Dataset Size, Update Frequency, Performance)                                                                                                                                                                                                                                                                                                                      | Example Use Case                                                                                                    |\n",
        "| :------------------- | :----------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------ |\n",
        "| **Patient Search**   |                          |                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                     |\n",
        "| By Patient ID (Exact Match)  | Hash Map (Dictionary)    | **Dataset Size:** O(1) average time complexity for lookup, insertion, and deletion, making it highly efficient for any dataset size (small to very large) where direct access by ID is needed. <br/> **Update Frequency:** Efficient for frequent additions/deletions/updates of patient records as these operations also average O(1). <br/> **Performance:** Provides near-instantaneous retrieval of specific patient details, crucial for real-time access by medical staff during emergencies. | Quickly retrieve a patient's full medical history using their unique patient ID at the admissions desk or ER. |\n",
        "| By Patient Name (Partial/Fuzzy Match) | Trie (Prefix Tree) / Full-text Search Engine (e.g., Elasticsearch, Lucene) | **Dataset Size:** Trie is efficient for prefix searches on large datasets (O(L) where L is query length). Full-text search engines handle very large, unstructured text data. <br/> **Update Frequency:** Trie updates are O(L) for insertion/deletion. Full-text search engines are optimized for frequent indexing and searching on dynamic data. <br/> **Performance:** Trie offers fast auto-completion for patient names. Full-text search provides highly relevant results for complex queries, synonym matching, and fault tolerance (typos) on patient demographics. | A doctor searching for a patient by a partial name input or to find all patients with a specific condition mentioned in their notes.        |\n",
        "| **Patient Sorting**  |                          |                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                     |\n",
        "| By Severity Level    | Merge Sort / Timsort (Python's default) | **Dataset Size:** O(N log N) time complexity, which is efficient and scales well for large datasets. It's a stable sort. <br/> **Update Frequency:** Severity levels can change, requiring re-sorting for up-to-date views. For very high frequency, consider keeping a dynamically sorted structure. <br/> **Performance:** Guarantees O(N log N) worst-case performance, suitable for consistently prioritizing patients in queues or displays. | Ordering patients in an emergency room queue from most severe to least severe. |\n",
        "| By Bill Amount       | Merge Sort / Timsort (Python's default) | **Dataset Size:** O(N log N) time complexity, effective for sorting financial records of varying sizes. <br/> **Update Frequency:** Similar to severity; bill amounts can change frequently with new services. <br/> **Performance:** Reliable performance for sorting financial data, enabling efficient analysis of outstanding bills or revenue generation. | Generating a report of patient bills from highest to lowest amount for accounting purposes or insurance claims.        |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4562b39"
      },
      "source": [
        "## Justification Table for Algorithms (Hospital Scenario)\n",
        "\n",
        "### Subtask:\n",
        "Create a markdown table outlining the recommended algorithms for patient record search (by ID and by name) and sorting (by severity level and by bill amount), along with justifications for each choice based on dataset size, update frequency, and performance requirements.\n",
        "\n",
        "### Recommended Algorithms and Justifications\n",
        "\n",
        "| Feature              | Algorithm Recommendation | Justification (Dataset Size, Update Frequency, Performance)                                                                                                                                                                                                                                                                                                                      | Example Use Case                                                                                                    |\n",
        "| :------------------- | :----------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------ |\n",
        "| **Patient Search**   |                          |                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                     |\n",
        "| By Patient ID (Exact Match)  | Hash Map (Dictionary)    | **Dataset Size:** O(1) average time complexity for lookup, insertion, and deletion, making it highly efficient for any dataset size (small to very large) where direct access by ID is needed. <br/> **Update Frequency:** Efficient for frequent additions/deletions/updates of patient records as these operations also average O(1). <br/> **Performance:** Provides near-instantaneous retrieval of specific patient details, crucial for real-time access by medical staff during emergencies. | Quickly retrieve a patient's full medical history using their unique patient ID at the admissions desk or ER. |\n",
        "| By Patient Name (Partial/Fuzzy Match) | Trie (Prefix Tree) / Full-text Search Engine (e.g., Elasticsearch, Lucene) | **Dataset Size:** Trie is efficient for prefix searches on large datasets (O(L) where L is query length). Full-text search engines handle very large, unstructured text data. <br/> **Update Frequency:** Trie updates are O(L) for insertion/deletion. Full-text search engines are optimized for frequent indexing and searching on dynamic data. <br/> **Performance:** Trie offers fast auto-completion for patient names. Full-text search provides highly relevant results for complex queries, synonym matching, and fault tolerance (typos) on patient demographics. | A doctor searching for a patient by a partial name input or to find all patients with a specific condition mentioned in their notes.        |\n",
        "| **Patient Sorting**  |                          |                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                     |\n",
        "| By Severity Level    | Merge Sort / Timsort (Python's default) | **Dataset Size:** O(N log N) time complexity, which is efficient and scales well for large datasets. It's a stable sort. <br/> **Update Frequency:** Severity levels can change, requiring re-sorting for up-to-date views. For very high frequency, consider keeping a dynamically sorted structure. <br/> **Performance:** Guarantees O(N log N) worst-case performance, suitable for consistently prioritizing patients in queues or displays. | Ordering patients in an emergency room queue from most severe to least severe. |\n",
        "| By Bill Amount       | Merge Sort / Timsort (Python's default) | **Dataset Size:** O(N log N) time complexity, effective for sorting financial records of varying sizes. <br/> **Update Frequency:** Similar to severity; bill amounts can change frequently with new services. <br/> **Performance:** Reliable performance for sorting financial data, enabling efficient analysis of outstanding bills or revenue generation. | Generating a report of patient bills from highest to lowest amount for accounting purposes or insurance claims.        |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "655feacc"
      },
      "source": [
        "## Justification Table for Algorithms (Hospital Scenario)\n",
        "\n",
        "### Subtask:\n",
        "Create a markdown table outlining the recommended algorithms for patient record search (by ID and by name) and sorting (by severity level and by bill amount), along with justifications for each choice based on dataset size, update frequency, and performance requirements.\n",
        "\n",
        "### Recommended Algorithms and Justifications\n",
        "\n",
        "| Feature              | Algorithm Recommendation | Justification (Dataset Size, Update Frequency, Performance)                                                                                                                                                                                                                                                                                                                      | Example Use Case                                                                                                    |\n",
        "| :------------------- | :----------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------ |\n",
        "| **Patient Search**   |                          |                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                     |\n",
        "| By Patient ID (Exact Match)  | Hash Map (Dictionary)    | **Dataset Size:** O(1) average time complexity for lookup, insertion, and deletion, making it highly efficient for any dataset size (small to very large) where direct access by ID is needed. <br/> **Update Frequency:** Efficient for frequent additions/deletions/updates of patient records as these operations also average O(1). <br/> **Performance:** Provides near-instantaneous retrieval of specific patient details, crucial for real-time access by medical staff during emergencies. | Quickly retrieve a patient's full medical history using their unique patient ID at the admissions desk or ER. |\n",
        "| By Patient Name (Partial/Fuzzy Match) | Trie (Prefix Tree) / Full-text Search Engine (e.g., Elasticsearch, Lucene) | **Dataset Size:** Trie is efficient for prefix searches on large datasets (O(L) where L is query length). Full-text search engines handle very large, unstructured text data. <br/> **Update Frequency:** Trie updates are O(L) for insertion/deletion. Full-text search engines are optimized for frequent indexing and searching on dynamic data. <br/> **Performance:** Trie offers fast auto-completion for patient names. Full-text search provides highly relevant results for complex queries, synonym matching, and fault tolerance (typos) on patient demographics. | A doctor searching for a patient by a partial name input or to find all patients with a specific condition mentioned in their notes.        |\n",
        "| **Patient Sorting**  |                          |                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                     |\n",
        "| By Severity Level    | Merge Sort / Timsort (Python's default) | **Dataset Size:** O(N log N) time complexity, which is efficient and scales well for large datasets. It's a stable sort. <br/> **Update Frequency:** Severity levels can change, requiring re-sorting for up-to-date views. For very high frequency, consider keeping a dynamically sorted structure. <br/> **Performance:** Guarantees O(N log N) worst-case performance, suitable for consistently prioritizing patients in queues or displays. | Ordering patients in an emergency room queue from most severe to least severe. |\n",
        "| By Bill Amount       | Merge Sort / Timsort (Python's default) | **Dataset Size:** O(N log N) time complexity, effective for sorting financial records of varying sizes. <br/> **Update Frequency:** Similar to severity; bill amounts can change frequently with new services. <br/> **Performance:** Reliable performance for sorting financial data, enabling efficient analysis of outstanding bills or revenue generation. | Generating a report of patient bills from highest to lowest amount for accounting purposes or insurance claims.        |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d834f2b4"
      },
      "source": [
        "## Justification Table for Algorithms (Hospital Scenario)\n",
        "\n",
        "### Subtask:\n",
        "Create a markdown table outlining the recommended algorithms for patient record search (by ID and by name) and sorting (by severity level and by bill amount), along with justifications for each choice based on dataset size, update frequency, and performance requirements.\n",
        "\n",
        "### Recommended Algorithms and Justifications\n",
        "\n",
        "| Feature              | Algorithm Recommendation | Justification (Dataset Size, Update Frequency, Performance)                                                                                                                                                                                                                                                                                                                      | Example Use Case                                                                                                    |\n",
        "| :------------------- | :----------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------ |\n",
        "| **Patient Search**   |                          |                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                     |\n",
        "| By Patient ID (Exact Match)  | Hash Map (Dictionary)    | **Dataset Size:** O(1) average time complexity for lookup, insertion, and deletion, making it highly efficient for any dataset size (small to very large) where direct access by ID is needed. <br/> **Update Frequency:** Efficient for frequent additions/deletions/updates of patient records as these operations also average O(1). <br/> **Performance:** Provides near-instantaneous retrieval of specific patient details, crucial for real-time access by medical staff during emergencies. | Quickly retrieve a patient's full medical history using their unique patient ID at the admissions desk or ER. |\n",
        "| By Patient Name (Partial/Fuzzy Match) | Trie (Prefix Tree) / Full-text Search Engine (e.g., Elasticsearch, Lucene) | **Dataset Size:** Trie is efficient for prefix searches on large datasets (O(L) where L is query length). Full-text search engines handle very large, unstructured text data. <br/> **Update Frequency:** Trie updates are O(L) for insertion/deletion. Full-text search engines are optimized for frequent indexing and searching on dynamic data. <br/> **Performance:** Trie offers fast auto-completion for patient names. Full-text search provides highly relevant results for complex queries, synonym matching, and fault tolerance (typos) on patient demographics. | A doctor searching for a patient by a partial name input or to find all patients with a specific condition mentioned in their notes.        |\n",
        "| **Patient Sorting**  |                          |                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                     |\n",
        "| By Severity Level    | Merge Sort / Timsort (Python's default) | **Dataset Size:** O(N log N) time complexity, which is efficient and scales well for large datasets. It's a stable sort. <br/> **Update Frequency:** Severity levels can change, requiring re-sorting for up-to-date views. For very high frequency, consider keeping a dynamically sorted structure. <br/> **Performance:** Guarantees O(N log N) worst-case performance, suitable for consistently prioritizing patients in queues or displays. | Ordering patients in an emergency room queue from most severe to least severe. |\n",
        "| By Bill Amount       | Merge Sort / Timsort (Python's default) | **Dataset Size:** O(N log N) time complexity, effective for sorting financial records of varying sizes. <br/> **Update Frequency:** Similar to severity; bill amounts can change frequently with new services. <br/> **Performance:** Reliable performance for sorting financial data, enabling efficient analysis of outstanding bills or revenue generation. | Generating a report of patient bills from highest to lowest amount for accounting purposes or insurance claims.        |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2005510"
      },
      "source": [
        "## Justification Table for Algorithms (Hospital Scenario)\n",
        "\n",
        "### Subtask:\n",
        "Create a markdown table outlining the recommended algorithms for patient record search (by ID and by name) and sorting (by severity level and by bill amount), along with justifications for each choice based on dataset size, update frequency, and performance requirements.\n",
        "\n",
        "### Recommended Algorithms and Justifications\n",
        "\n",
        "| Feature              | Algorithm Recommendation | Justification (Dataset Size, Update Frequency, Performance)                                                                                                                                                                                                                                                                                                                      | Example Use Case                                                                                                    |\n",
        "| :------------------- | :----------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------ |\n",
        "| **Patient Search**   |                          |                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                     |\n",
        "| By Patient ID (Exact Match)  | Hash Map (Dictionary)    | **Dataset Size:** O(1) average time complexity for lookup, insertion, and deletion, making it highly efficient for any dataset size (small to very large) where direct access by ID is needed. <br/> **Update Frequency:** Efficient for frequent additions/deletions/updates of patient records as these operations also average O(1). <br/> **Performance:** Provides near-instantaneous retrieval of specific patient details, crucial for real-time access by medical staff during emergencies. | Quickly retrieve a patient's full medical history using their unique patient ID at the admissions desk or ER. |\n",
        "| By Patient Name (Partial/Fuzzy Match) | Trie (Prefix Tree) / Full-text Search Engine (e.g., Elasticsearch, Lucene) | **Dataset Size:** Trie is efficient for prefix searches on large datasets (O(L) where L is query length). Full-text search engines handle very large, unstructured text data. <br/> **Update Frequency:** Trie updates are O(L) for insertion/deletion. Full-text search engines are optimized for frequent indexing and searching on dynamic data. <br/> **Performance:** Trie offers fast auto-completion for patient names. Full-text search provides highly relevant results for complex queries, synonym matching, and fault tolerance (typos) on patient demographics. | A doctor searching for a patient by a partial name input or to find all patients with a specific condition mentioned in their notes.        |\n",
        "| **Patient Sorting**  |                          |                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                     |\n",
        "| By Severity Level    | Merge Sort / Timsort (Python's default) | **Dataset Size:** O(N log N) time complexity, which is efficient and scales well for large datasets. It's a stable sort. <br/> **Update Frequency:** Severity levels can change, requiring re-sorting for up-to-date views. For very high frequency, consider keeping a dynamically sorted structure. <br/> **Performance:** Guarantees O(N log N) worst-case performance, suitable for consistently prioritizing patients in queues or displays. | Ordering patients in an emergency room queue from most severe to least severe. |\n",
        "| By Bill Amount       | Merge Sort / Timsort (Python's default) | **Dataset Size:** O(N log N) time complexity, effective for sorting financial records of varying sizes. <br/> **Update Frequency:** Similar to severity; bill amounts can change frequently with new services. <br/> **Performance:** Reliable performance for sorting financial data, enabling efficient analysis of outstanding bills or revenue generation. | Generating a report of patient bills from highest to lowest amount for accounting purposes or insurance claims.        |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff24ddae"
      },
      "source": [
        "## Justification Table for Algorithms (Hospital Scenario)\n",
        "\n",
        "### Subtask:\n",
        "Create a markdown table outlining the recommended algorithms for patient record search (by ID and by name) and sorting (by severity level and by bill amount), along with justifications for each choice based on dataset size, update frequency, and performance requirements.\n",
        "\n",
        "### Recommended Algorithms and Justifications\n",
        "\n",
        "| Feature              | Algorithm Recommendation | Justification (Dataset Size, Update Frequency, Performance)                                                                                                                                                                                                                                                                                                                      | Example Use Case                                                                                                    |\n",
        "| :------------------- | :----------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------ |\n",
        "| **Patient Search**   |                          |                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                     |\n",
        "| By Patient ID (Exact Match)  | Hash Map (Dictionary)    | **Dataset Size:** O(1) average time complexity for lookup, insertion, and deletion, making it highly efficient for any dataset size (small to very large) where direct access by ID is needed. <br/> **Update Frequency:** Efficient for frequent additions/deletions/updates of patient records as these operations also average O(1). <br/> **Performance:** Provides near-instantaneous retrieval of specific patient details, crucial for real-time access by medical staff during emergencies. | Quickly retrieve a patient's full medical history using their unique patient ID at the admissions desk or ER. |\n",
        "| By Patient Name (Partial/Fuzzy Match) | Trie (Prefix Tree) / Full-text Search Engine (e.g., Elasticsearch, Lucene) | **Dataset Size:** Trie is efficient for prefix searches on large datasets (O(L) where L is query length). Full-text search engines handle very large, unstructured text data. <br/> **Update Frequency:** Trie updates are O(L) for insertion/deletion. Full-text search engines are optimized for frequent indexing and searching on dynamic data. <br/> **Performance:** Trie offers fast auto-completion for patient names. Full-text search provides highly relevant results for complex queries, synonym matching, and fault tolerance (typos) on patient demographics. | A doctor searching for a patient by a partial name input or to find all patients with a specific condition mentioned in their notes.        |\n",
        "| **Patient Sorting**  |                          |                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                     |\n",
        "| By Severity Level    | Merge Sort / Timsort (Python's default) | **Dataset Size:** O(N log N) time complexity, which is efficient and scales well for large datasets. It's a stable sort. <br/> **Update Frequency:** Severity levels can change, requiring re-sorting for up-to-date views. For very high frequency, consider keeping a dynamically sorted structure. <br/> **Performance:** Guarantees O(N log N) worst-case performance, suitable for consistently prioritizing patients in queues or displays. | Ordering patients in an emergency room queue from most severe to least severe. |\n",
        "| By Bill Amount       | Merge Sort / Timsort (Python's default) | **Dataset Size:** O(N log N) time complexity, effective for sorting financial records of varying sizes. <br/> **Update Frequency:** Similar to severity; bill amounts can change frequently with new services. <br/> **Performance:** Reliable performance for sorting financial data, enabling efficient analysis of outstanding bills or revenue generation. | Generating a report of patient bills from highest to lowest amount for accounting purposes or insurance claims.        |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39cf6c46"
      },
      "source": [
        "## Justification Table for Algorithms (Hospital Scenario)\n",
        "\n",
        "### Subtask:\n",
        "Create a markdown table outlining the recommended algorithms for patient record search (by ID and by name) and sorting (by severity level and by bill amount), along with justifications for each choice based on dataset size, update frequency, and performance requirements.\n",
        "\n",
        "### Recommended Algorithms and Justifications\n",
        "\n",
        "| Feature              | Algorithm Recommendation | Justification (Dataset Size, Update Frequency, Performance)                                                                                                                                                                                                                                                                                                                      | Example Use Case                                                                                                    |\n",
        "| :------------------- | :----------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------ |\n",
        "| **Patient Search**   |                          |                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                     |\n",
        "| By Patient ID (Exact Match)  | Hash Map (Dictionary)    | **Dataset Size:** O(1) average time complexity for lookup, insertion, and deletion, making it highly efficient for any dataset size (small to very large) where direct access by ID is needed. <br/> **Update Frequency:** Efficient for frequent additions/deletions/updates of patient records as these operations also average O(1). <br/> **Performance:** Provides near-instantaneous retrieval of specific patient details, crucial for real-time access by medical staff during emergencies. | Quickly retrieve a patient's full medical history using their unique patient ID at the admissions desk or ER. |\n",
        "| By Patient Name (Partial/Fuzzy Match) | Trie (Prefix Tree) / Full-text Search Engine (e.g., Elasticsearch, Lucene) | **Dataset Size:** Trie is efficient for prefix searches on large datasets (O(L) where L is query length). Full-text search engines handle very large, unstructured text data. <br/> **Update Frequency:** Trie updates are O(L) for insertion/deletion. Full-text search engines are optimized for frequent indexing and searching on dynamic data. <br/> **Performance:** Trie offers fast auto-completion for patient names. Full-text search provides highly relevant results for complex queries, synonym matching, and fault tolerance (typos) on patient demographics. | A doctor searching for a patient by a partial name input or to find all patients with a specific condition mentioned in their notes.        |\n",
        "| **Patient Sorting**  |                          |                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                     |\n",
        "| By Severity Level    | Merge Sort / Timsort (Python's default) | **Dataset Size:** O(N log N) time complexity, which is efficient and scales well for large datasets. It's a stable sort. <br/> **Update Frequency:** Severity levels can change, requiring re-sorting for up-to-date views. For very high frequency, consider keeping a dynamically sorted structure. <br/> **Performance:** Guarantees O(N log N) worst-case performance, suitable for consistently prioritizing patients in queues or displays. | Ordering patients in an emergency room queue from most severe to least severe. |\n",
        "| By Bill Amount       | Merge Sort / Timsort (Python's default) | **Dataset Size:** O(N log N) time complexity, effective for sorting financial records of varying sizes. <br/> **Update Frequency:** Similar to severity; bill amounts can change frequently with new services. <br/> **Performance:** Reliable performance for sorting financial data, enabling efficient analysis of outstanding bills or revenue generation. | Generating a report of patient bills from highest to lowest amount for accounting purposes or insurance claims.        |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51c588d6"
      },
      "source": [
        "## Justification Table for Algorithms (Hospital Scenario)\n",
        "\n",
        "### Subtask:\n",
        "Create a markdown table outlining the recommended algorithms for patient record search (by ID and by name) and sorting (by severity level and by bill amount), along with justifications for each choice based on dataset size, update frequency, and performance requirements.\n",
        "\n",
        "### Recommended Algorithms and Justifications\n",
        "\n",
        "| Feature              | Algorithm Recommendation | Justification (Dataset Size, Update Frequency, Performance)                                                                                                                                                                                                                                                                                                                      | Example Use Case                                                                                                    |\n",
        "| :------------------- | :----------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------ |\n",
        "| **Patient Search**   |                          |                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                     |\n",
        "| By Patient ID (Exact Match)  | Hash Map (Dictionary)    | **Dataset Size:** O(1) average time complexity for lookup, insertion, and deletion, making it highly efficient for any dataset size (small to very large) where direct access by ID is needed. <br/> **Update Frequency:** Efficient for frequent additions/deletions/updates of patient records as these operations also average O(1). <br/> **Performance:** Provides near-instantaneous retrieval of specific patient details, crucial for real-time access by medical staff during emergencies. | Quickly retrieve a patient's full medical history using their unique patient ID at the admissions desk or ER. |\n",
        "| By Patient Name (Partial/Fuzzy Match) | Trie (Prefix Tree) / Full-text Search Engine (e.g., Elasticsearch, Lucene) | **Dataset Size:** Trie is efficient for prefix searches on large datasets (O(L) where L is query length). Full-text search engines handle very large, unstructured text data. <br/> **Update Frequency:** Trie updates are O(L) for insertion/deletion. Full-text search engines are optimized for frequent indexing and searching on dynamic data. <br/> **Performance:** Trie offers fast auto-completion for patient names. Full-text search provides highly relevant results for complex queries, synonym matching, and fault tolerance (typos) on patient demographics. | A doctor searching for a patient by a partial name input or to find all patients with a specific condition mentioned in their notes.        |\n",
        "| **Patient Sorting**  |                          |                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                     |\n",
        "| By Severity Level    | Merge Sort / Timsort (Python's default) | **Dataset Size:** O(N log N) time complexity, which is efficient and scales well for large datasets. It's a stable sort. <br/> **Update Frequency:** Severity levels can change, requiring re-sorting for up-to-date views. For very high frequency, consider keeping a dynamically sorted structure. <br/> **Performance:** Guarantees O(N log N) worst-case performance, suitable for consistently prioritizing patients in queues or displays. | Ordering patients in an emergency room queue from most severe to least severe. |\n",
        "| By Bill Amount       | Merge Sort / Timsort (Python's default) | **Dataset Size:** O(N log N) time complexity, effective for sorting financial records of varying sizes. <br/> **Update Frequency:** Similar to severity; bill amounts can change frequently with new services. <br/> **Performance:** Reliable performance for sorting financial data, enabling efficient analysis of outstanding bills or revenue generation. | Generating a report of patient bills from highest to lowest amount for accounting purposes or insurance claims.        |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeaa172f"
      },
      "source": [
        "## Define Sample Patient Data\n",
        "\n",
        "### Subtask:\n",
        "Create a sample list of patient dictionaries, each containing 'patient_id', 'name', 'severity_level', 'admission_date', and 'bill_amount' attributes, to serve as the dataset for demonstrating search and sort functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8fe2559"
      },
      "source": [
        "**Reasoning**:\n",
        "To fulfill the subtask, I need to create a Python list of patient dictionaries as specified in the instructions. This `code_block` will define the `patients` list with sample data and then print it for verification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "b69cb220",
        "outputId": "d0f1d9f1-04a9-499c-c89c-9bc847f80a99"
      },
      "source": [
        "import datetime\n",
        "\n",
        "patients = [\n",
        "    {'patient_id': 'PAT001', 'name': 'Alice Smith', 'severity_level': 4, 'admission_date': '2023-01-15', 'bill_amount': 1500.75},\n",
        "    {'patient_id': 'PAT002', 'name': 'Bob Johnson', 'severity_level': 2, 'admission_date': '2023-01-20', 'bill_amount': 300.50},\n",
        "    {'patient_id': 'PAT003', 'name': 'Charlie Brown', 'severity_level': 5, 'admission_date': '2023-01-18', 'bill_amount': 3500.00},\n",
        "    {'patient_id': 'PAT004', 'name': 'Diana Prince', 'severity_level': 3, 'admission_date': '2023-02-01', 'bill_amount': 800.20},\n",
        "    {'patient_id': 'PAT005', 'name': 'Eve Adams', 'severity_level': 1, 'admission_date': '2023-01-25', 'bill_amount': 120.00},\n",
        "    {'patient_id': 'PAT006', 'name': 'Frank White', 'severity_level': 4, 'admission_date': '2023-02-05', 'bill_amount': 2100.90},\n",
        "    {'patient_id': 'PAT007', 'name': 'Grace Lee', 'severity_level': 2, 'admission_date': '2023-01-30', 'bill_amount': 650.30}\n",
        "]\n",
        "\n",
        "print(\"Sample patient data created:\")\n",
        "for patient in patients:\n",
        "    print(patient)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample patient data created:\n",
            "{'patient_id': 'PAT001', 'name': 'Alice Smith', 'severity_level': 4, 'admission_date': '2023-01-15', 'bill_amount': 1500.75}\n",
            "{'patient_id': 'PAT002', 'name': 'Bob Johnson', 'severity_level': 2, 'admission_date': '2023-01-20', 'bill_amount': 300.5}\n",
            "{'patient_id': 'PAT003', 'name': 'Charlie Brown', 'severity_level': 5, 'admission_date': '2023-01-18', 'bill_amount': 3500.0}\n",
            "{'patient_id': 'PAT004', 'name': 'Diana Prince', 'severity_level': 3, 'admission_date': '2023-02-01', 'bill_amount': 800.2}\n",
            "{'patient_id': 'PAT005', 'name': 'Eve Adams', 'severity_level': 1, 'admission_date': '2023-01-25', 'bill_amount': 120.0}\n",
            "{'patient_id': 'PAT006', 'name': 'Frank White', 'severity_level': 4, 'admission_date': '2023-02-05', 'bill_amount': 2100.9}\n",
            "{'patient_id': 'PAT007', 'name': 'Grace Lee', 'severity_level': 2, 'admission_date': '2023-01-30', 'bill_amount': 650.3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffa58200"
      },
      "source": [
        "## Implement Efficient Search by Patient ID\n",
        "\n",
        "### Subtask:\n",
        "Implement a Python function that efficiently searches for a patient record by its unique 'patient_id'. This function will use a dictionary (hash map) for O(1) average time complexity lookups, demonstrating the most efficient approach for direct ID access."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9638a9c5"
      },
      "source": [
        "## Implement Efficient Search by Patient ID\n",
        "\n",
        "### Subtask:\n",
        "Implement a Python function that efficiently searches for a patient record by its unique 'patient_id'. This function will use a dictionary (hash map) for O(1) average time complexity lookups, demonstrating the most efficient approach for direct ID access."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae77d4b1"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask now requires the implementation of the `search_patient_by_id` function using a hash map, as outlined in the previous markdown step. This code block will define the function, create a hash map from the `patients` list, and include comprehensive test cases to verify its correctness.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "220c9b67",
        "outputId": "f70ba451-a774-42e1-ad45-414a73dfc4d6"
      },
      "source": [
        "def search_patient_by_id(patients_list, patient_id_to_find):\n",
        "    \"\"\"\n",
        "    Searches for a patient by their unique patient_id using a hash map (dictionary).\n",
        "\n",
        "    Args:\n",
        "        patients_list (list): A list of patient dictionaries.\n",
        "        patient_id_to_find (str): The patient_id to search for.\n",
        "\n",
        "    Returns:\n",
        "        dict or None: The patient dictionary if found, otherwise None.\n",
        "    \"\"\"\n",
        "    # Create a hash map (dictionary) for O(1) average time complexity lookups\n",
        "    patient_hash_map = {}\n",
        "    for patient in patients_list:\n",
        "        patient_hash_map[patient['patient_id']] = patient\n",
        "\n",
        "    # Look up the patient_id in the hash map\n",
        "    return patient_hash_map.get(patient_id_to_find)\n",
        "\n",
        "# Test cases\n",
        "print(\"--- Testing search_patient_by_id ---\")\n",
        "\n",
        "# Test with an existing patient_id\n",
        "existing_id_patient = 'PAT001'\n",
        "found_patient = search_patient_by_id(patients, existing_id_patient)\n",
        "print(f\"Searching for Patient ID '{existing_id_patient}': {found_patient}\")\n",
        "assert found_patient is not None and found_patient['patient_id'] == existing_id_patient, f\"Test failed for existing ID {existing_id_patient}\"\n",
        "print(f\"  Test Passed for existing ID: {existing_id_patient}\")\n",
        "\n",
        "# Test with another existing patient_id\n",
        "existing_id_patient_2 = 'PAT005'\n",
        "found_patient_2 = search_patient_by_id(patients, existing_id_patient_2)\n",
        "print(f\"Searching for Patient ID '{existing_id_patient_2}': {found_patient_2}\")\n",
        "assert found_patient_2 is not None and found_patient_2['patient_id'] == existing_id_patient_2, f\"Test failed for existing ID {existing_id_patient_2}\"\n",
        "print(f\"  Test Passed for existing ID: {existing_id_patient_2}\")\n",
        "\n",
        "# Test with a non-existent patient_id\n",
        "non_existent_id_patient = 'PAT999'\n",
        "not_found_patient = search_patient_by_id(patients, non_existent_id_patient)\n",
        "print(f\"Searching for Patient ID '{non_existent_id_patient}': {not_found_patient}\")\n",
        "assert not_found_patient is None, f\"Test failed for non-existent ID {non_existent_id_patient}\"\n",
        "print(f\"  Test Passed for non-existent ID: {non_existent_id_patient}\")\n",
        "\n",
        "print(\"All test cases passed for search_patient_by_id!\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Testing search_patient_by_id ---\n",
            "Searching for Patient ID 'PAT001': {'patient_id': 'PAT001', 'name': 'Alice Smith', 'severity_level': 4, 'admission_date': '2023-01-15', 'bill_amount': 1500.75}\n",
            "  Test Passed for existing ID: PAT001\n",
            "Searching for Patient ID 'PAT005': {'patient_id': 'PAT005', 'name': 'Eve Adams', 'severity_level': 1, 'admission_date': '2023-01-25', 'bill_amount': 120.0}\n",
            "  Test Passed for existing ID: PAT005\n",
            "Searching for Patient ID 'PAT999': None\n",
            "  Test Passed for non-existent ID: PAT999\n",
            "All test cases passed for search_patient_by_id!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0fca958"
      },
      "source": [
        "## Implement Efficient Search by Patient Name\n",
        "\n",
        "### Subtask:\n",
        "Implement a Python function that searches for a patient record by their 'name'. This will involve using the existing `binary_search` function on a list sorted by patient name, similar to how product search by name was implemented for the retail inventory system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bd9d124"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous markdown block outlined the subtask of implementing search by patient name. This code block will now define the `search_patient_by_name` function, which will sort the patient list by name and then use the previously defined `binary_search` function to find the target patient, along with comprehensive test cases to verify its correctness.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4cddfd2e",
        "outputId": "5b2b98b7-aa28-4015-8976-bebbe00079f5"
      },
      "source": [
        "def search_patient_by_name(patients_list, target_name):\n",
        "    \"\"\"\n",
        "    Searches for a patient by their name using binary search on a sorted list.\n",
        "\n",
        "    Args:\n",
        "        patients_list (list): A list of patient dictionaries.\n",
        "        target_name (str): The patient name to search for.\n",
        "\n",
        "    Returns:\n",
        "        dict or None: The patient dictionary if found, otherwise None.\n",
        "    \"\"\"\n",
        "    # 1. Sort the patients list by 'name' attribute\n",
        "    # For simplicity, we'll assume exact case-sensitive matching for now.\n",
        "    sorted_patients = sorted(patients_list, key=lambda x: x['name'])\n",
        "\n",
        "    # 2. Create a separate list containing only the 'name' values from the sorted patient list\n",
        "    patient_names = [patient['name'] for patient in sorted_patients]\n",
        "\n",
        "    # 3. Utilize the previously defined binary_search function\n",
        "    index = binary_search(patient_names, target_name)\n",
        "\n",
        "    # 4. If the binary_search returns a valid index, retrieve the corresponding patient dictionary\n",
        "    if index != -1:\n",
        "        return sorted_patients[index]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Test cases\n",
        "print(\"--- Testing search_patient_by_name ---\")\n",
        "\n",
        "test_cases_patient_name = [\n",
        "    ('Alice Smith', {'patient_id': 'PAT001', 'name': 'Alice Smith', 'severity_level': 4, 'admission_date': '2023-01-15', 'bill_amount': 1500.75}), # Exact match\n",
        "    ('Eve Adams', {'patient_id': 'PAT005', 'name': 'Eve Adams', 'severity_level': 1, 'admission_date': '2023-01-25', 'bill_amount': 120.00}),   # Another exact match\n",
        "    ('Diana Prince', {'patient_id': 'PAT004', 'name': 'Diana Prince', 'severity_level': 3, 'admission_date': '2023-02-01', 'bill_amount': 800.20}), # Another exact match\n",
        "    ('John Doe', None), # Non-existent name\n",
        "    ('alice smith', None), # Case-sensitive (should not find if 'Alice Smith' is the actual name)\n",
        "    ('Bob Johnson', {'patient_id': 'PAT002', 'name': 'Bob Johnson', 'severity_level': 2, 'admission_date': '2023-01-20', 'bill_amount': 300.50}),\n",
        "    ('Grace Lee', {'patient_id': 'PAT007', 'name': 'Grace Lee', 'severity_level': 2, 'admission_date': '2023-01-30', 'bill_amount': 650.30})\n",
        "]\n",
        "\n",
        "for target_name, expected_patient in test_cases_patient_name:\n",
        "    result = search_patient_by_name(patients, target_name)\n",
        "    print(f\"Searching for name '{target_name}'. Expected: {expected_patient}, Got: {result}\")\n",
        "    assert result == expected_patient, f\"Test failed for name '{target_name}'. Expected {expected_patient}, got {result}\"\n",
        "    print(f\"  Test Passed for name: {target_name}\")\n",
        "\n",
        "print(\"All test cases passed for search_patient_by_name!\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Testing search_patient_by_name ---\n",
            "Searching for name 'Alice Smith'. Expected: {'patient_id': 'PAT001', 'name': 'Alice Smith', 'severity_level': 4, 'admission_date': '2023-01-15', 'bill_amount': 1500.75}, Got: {'patient_id': 'PAT001', 'name': 'Alice Smith', 'severity_level': 4, 'admission_date': '2023-01-15', 'bill_amount': 1500.75}\n",
            "  Test Passed for name: Alice Smith\n",
            "Searching for name 'Eve Adams'. Expected: {'patient_id': 'PAT005', 'name': 'Eve Adams', 'severity_level': 1, 'admission_date': '2023-01-25', 'bill_amount': 120.0}, Got: {'patient_id': 'PAT005', 'name': 'Eve Adams', 'severity_level': 1, 'admission_date': '2023-01-25', 'bill_amount': 120.0}\n",
            "  Test Passed for name: Eve Adams\n",
            "Searching for name 'Diana Prince'. Expected: {'patient_id': 'PAT004', 'name': 'Diana Prince', 'severity_level': 3, 'admission_date': '2023-02-01', 'bill_amount': 800.2}, Got: {'patient_id': 'PAT004', 'name': 'Diana Prince', 'severity_level': 3, 'admission_date': '2023-02-01', 'bill_amount': 800.2}\n",
            "  Test Passed for name: Diana Prince\n",
            "Searching for name 'John Doe'. Expected: None, Got: None\n",
            "  Test Passed for name: John Doe\n",
            "Searching for name 'alice smith'. Expected: None, Got: None\n",
            "  Test Passed for name: alice smith\n",
            "Searching for name 'Bob Johnson'. Expected: {'patient_id': 'PAT002', 'name': 'Bob Johnson', 'severity_level': 2, 'admission_date': '2023-01-20', 'bill_amount': 300.5}, Got: {'patient_id': 'PAT002', 'name': 'Bob Johnson', 'severity_level': 2, 'admission_date': '2023-01-20', 'bill_amount': 300.5}\n",
            "  Test Passed for name: Bob Johnson\n",
            "Searching for name 'Grace Lee'. Expected: {'patient_id': 'PAT007', 'name': 'Grace Lee', 'severity_level': 2, 'admission_date': '2023-01-30', 'bill_amount': 650.3}, Got: {'patient_id': 'PAT007', 'name': 'Grace Lee', 'severity_level': 2, 'admission_date': '2023-01-30', 'bill_amount': 650.3}\n",
            "  Test Passed for name: Grace Lee\n",
            "All test cases passed for search_patient_by_name!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8396b3f3"
      },
      "source": [
        "## Demonstrate Patient Sorting by Severity Level and Bill Amount\n",
        "\n",
        "### Subtask:\n",
        "Demonstrate the usage of the existing `merge_sort` function to sort the sample patient data. This will involve sorting the list of patient dictionaries first by their 'severity_level' attribute (descending, as higher number usually means more severe) and then by their 'bill_amount' attribute (ascending), showcasing the algorithm's application."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f80fa554"
      },
      "source": [
        "## Demonstrate Patient Sorting by Severity Level and Bill Amount\n",
        "\n",
        "### Subtask:\n",
        "Demonstrate the usage of the existing `merge_sort` function to sort the sample patient data. This will involve sorting the list of patient dictionaries first by their 'severity_level' attribute (descending, as higher number usually means more severe) and then by their 'bill_amount' attribute (ascending), showcasing the algorithm's application."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2b9094b"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous markdown block outlined the subtask of demonstrating patient sorting. Now, I need to implement a Python function that uses the existing `merge_sort` to sort patient records by 'severity_level' and 'bill_amount' as specified, and include test cases.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "05c21a71",
        "outputId": "98b585e9-fce4-4550-dd5c-52c98bc167fe"
      },
      "source": [
        "def sort_patients_by_key(patients_list, key_name, reverse=False):\n",
        "    \"\"\"\n",
        "    Sorts a list of patient dictionaries using the existing merge_sort function\n",
        "    based on a specified key.\n",
        "\n",
        "    Args:\n",
        "        patients_list (list): A list of patient dictionaries.\n",
        "        key_name (str): The key in the dictionary to sort by (e.g., 'severity_level', 'bill_amount').\n",
        "        reverse (bool): If True, sort in descending order. Defaults to False (ascending).\n",
        "\n",
        "    Returns:\n",
        "        list: A new list of patient dictionaries, sorted by the specified key.\n",
        "    \"\"\"\n",
        "    # Create a temporary list of tuples (key_value, patient_dictionary)\n",
        "    items_to_sort = []\n",
        "    for patient in patients_list:\n",
        "        items_to_sort.append((patient[key_name], patient))\n",
        "\n",
        "    # Use the existing merge_sort function to sort these tuples\n",
        "    # merge_sort sorts in ascending order by default (based on tuple comparison)\n",
        "    sorted_items = merge_sort(items_to_sort)\n",
        "\n",
        "    # Extract the original patient dictionaries from the sorted tuples\n",
        "    sorted_patients = [item[1] for item in sorted_items]\n",
        "\n",
        "    if reverse:\n",
        "        return sorted_patients[::-1] # Reverse the list if descending order is requested\n",
        "    return sorted_patients\n",
        "\n",
        "# Test cases for sorting\n",
        "print(\"--- Demonstrating Patient Sorting by Severity Level and Bill Amount ---\")\n",
        "\n",
        "# 1. Sort by severity_level (descending: higher number = more severe)\n",
        "print(\"\\nSorting patients by 'severity_level' (descending):\")\n",
        "sorted_by_severity = sort_patients_by_key(patients, 'severity_level', reverse=True)\n",
        "for patient in sorted_by_severity:\n",
        "    print(patient)\n",
        "\n",
        "# Verify sorting by severity_level\n",
        "assert sorted_by_severity[0]['severity_level'] == 5, f\"Expected highest severity 5, got {sorted_by_severity[0]['severity_level']}\"\n",
        "assert sorted_by_severity[-1]['severity_level'] == 1, f\"Expected lowest severity 1, got {sorted_by_severity[-1]['severity_level']}\"\n",
        "print(\"  Verification by severity_level passed (first and last elements).\")\n",
        "\n",
        "# 2. Sort by bill_amount (ascending)\n",
        "print(\"\\nSorting patients by 'bill_amount' (ascending):\")\n",
        "# It's good practice to sort a copy if you need the original list untouched elsewhere\n",
        "sorted_by_bill_amount = sort_patients_by_key(patients.copy(), 'bill_amount', reverse=False)\n",
        "for patient in sorted_by_bill_amount:\n",
        "    print(patient)\n",
        "\n",
        "# Verify sorting by bill_amount\n",
        "assert sorted_by_bill_amount[0]['bill_amount'] == 120.00, f\"Expected lowest bill amount 120.00, got {sorted_by_bill_amount[0]['bill_amount']}\"\n",
        "assert sorted_by_bill_amount[-1]['bill_amount'] == 3500.00, f\"Expected highest bill amount 3500.00, got {sorted_by_bill_amount[-1]['bill_amount']}\"\n",
        "print(\"  Verification by bill_amount passed (first and last elements).\")\n",
        "\n",
        "print(\"\\nAll patient sorting demonstrations and verifications complete!\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Demonstrating Patient Sorting by Severity Level and Bill Amount ---\n",
            "\n",
            "Sorting patients by 'severity_level' (descending):\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'<' not supported between instances of 'dict' and 'dict'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4266073562.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# 1. Sort by severity_level (descending: higher number = more severe)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nSorting patients by 'severity_level' (descending):\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0msorted_by_severity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort_patients_by_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'severity_level'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpatient\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted_by_severity\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4266073562.py\u001b[0m in \u001b[0;36msort_patients_by_key\u001b[0;34m(patients_list, key_name, reverse)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Use the existing merge_sort function to sort these tuples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# merge_sort sorts in ascending order by default (based on tuple comparison)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0msorted_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_sort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems_to_sort\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Extract the original patient dictionaries from the sorted tuples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1749809598.py\u001b[0m in \u001b[0;36mmerge_sort\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mright_half\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_sort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_half\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_half\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_half\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1749809598.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mleft_idx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mright_idx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mright_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mmerged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mleft_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'dict' and 'dict'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69c4b43a"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because Python tried to compare two dictionaries directly when their primary sorting keys were equal, resulting in a `TypeError`. To fix this while reusing the existing `merge_sort` function, I will modify the `sort_patients_by_key` function. This modification involves including the original index of each patient as a secondary sorting key within the tuple passed to `merge_sort`. This ensures that if two patients have the same primary sorting key (e.g., same severity level), Python will compare their indices (which are integers and thus comparable) instead of attempting to compare the dictionary objects directly. This makes the tuples fully comparable, allowing `merge_sort` to function correctly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "e4bc5e42",
        "outputId": "9b76708e-8261-44fc-b5b1-1194f9db4e72"
      },
      "source": [
        "def sort_patients_by_key(patients_list, key_name, reverse=False):\n",
        "    \"\"\"\n",
        "    Sorts a list of patient dictionaries using the existing merge_sort function\n",
        "    based on a specified key.\n",
        "\n",
        "    Args:\n",
        "        patients_list (list): A list of patient dictionaries.\n",
        "        key_name (str): The key in the dictionary to sort by (e.g., 'severity_level', 'bill_amount').\n",
        "        reverse (bool): If True, sort in descending order. Defaults to False (ascending).\n",
        "\n",
        "    Returns:\n",
        "        list: A new list of patient dictionaries, sorted by the specified key.\n",
        "    \"\"\"\n",
        "    # Create a temporary list of tuples (key_value, original_index, patient_dictionary)\n",
        "    # The original_index ensures stable sorting and prevents direct dictionary comparison\n",
        "    # if key_values are equal, as Python will compare the indices next.\n",
        "    items_to_sort = []\n",
        "    for i, patient in enumerate(patients_list):\n",
        "        items_to_sort.append((patient[key_name], i, patient)) # Add index 'i' here\n",
        "\n",
        "    # Use the existing merge_sort function to sort these tuples\n",
        "    # merge_sort sorts in ascending order by default (based on tuple comparison)\n",
        "    sorted_items = merge_sort(items_to_sort)\n",
        "\n",
        "    # Extract the original patient dictionaries from the sorted tuples\n",
        "    sorted_patients = [item[2] for item in sorted_items] # Now extract from index 2\n",
        "\n",
        "    if reverse:\n",
        "        return sorted_patients[::-1] # Reverse the list if descending order is requested\n",
        "    return sorted_patients\n",
        "\n",
        "# Test cases\n",
        "print(\"--- Demonstrating Patient Sorting by Severity Level and Bill Amount ---\")\n",
        "\n",
        "# 1. Sort by severity_level (descending: higher number = more severe)\n",
        "print(\"\\nSorting patients by 'severity_level' (descending):\")\n",
        "sorted_by_severity = sort_patients_by_key(patients, 'severity_level', reverse=True)\n",
        "for patient in sorted_by_severity:\n",
        "    print(patient)\n",
        "\n",
        "# Verify sorting by severity_level\n",
        "assert sorted_by_severity[0]['severity_level'] == 5, f\"Expected highest severity 5, got {sorted_by_severity[0]['severity_level']}\"\n",
        "assert sorted_by_severity[-1]['severity_level'] == 1, f\"Expected lowest severity 1, got {sorted_by_severity[-1]['severity_level']}\"\n",
        "print(\"  Verification by severity_level passed (first and last elements).\")\n",
        "\n",
        "# 2. Sort by bill_amount (ascending)\n",
        "print(\"\\nSorting patients by 'bill_amount' (ascending):\")\n",
        "# It's good practice to sort a copy if you need the original list untouched elsewhere\n",
        "sorted_by_bill_amount = sort_patients_by_key(patients.copy(), 'bill_amount', reverse=False)\n",
        "for patient in sorted_by_bill_amount:\n",
        "    print(patient)\n",
        "\n",
        "# Verify sorting by bill_amount\n",
        "assert sorted_by_bill_amount[0]['bill_amount'] == 120.00, f\"Expected lowest bill amount 120.00, got {sorted_by_bill_amount[0]['bill_amount']}\"\n",
        "assert sorted_by_bill_amount[-1]['bill_amount'] == 3500.00, f\"Expected highest bill amount 3500.00, got {sorted_by_bill_amount[-1]['bill_amount']}\"\n",
        "print(\"  Verification by bill_amount passed (first and last elements).\")\n",
        "\n",
        "print(\"\\nAll patient sorting demonstrations and verifications complete!\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Demonstrating Patient Sorting by Severity Level and Bill Amount ---\n",
            "\n",
            "Sorting patients by 'severity_level' (descending):\n",
            "{'patient_id': 'PAT003', 'name': 'Charlie Brown', 'severity_level': 5, 'admission_date': '2023-01-18', 'bill_amount': 3500.0}\n",
            "{'patient_id': 'PAT006', 'name': 'Frank White', 'severity_level': 4, 'admission_date': '2023-02-05', 'bill_amount': 2100.9}\n",
            "{'patient_id': 'PAT001', 'name': 'Alice Smith', 'severity_level': 4, 'admission_date': '2023-01-15', 'bill_amount': 1500.75}\n",
            "{'patient_id': 'PAT004', 'name': 'Diana Prince', 'severity_level': 3, 'admission_date': '2023-02-01', 'bill_amount': 800.2}\n",
            "{'patient_id': 'PAT007', 'name': 'Grace Lee', 'severity_level': 2, 'admission_date': '2023-01-30', 'bill_amount': 650.3}\n",
            "{'patient_id': 'PAT002', 'name': 'Bob Johnson', 'severity_level': 2, 'admission_date': '2023-01-20', 'bill_amount': 300.5}\n",
            "{'patient_id': 'PAT005', 'name': 'Eve Adams', 'severity_level': 1, 'admission_date': '2023-01-25', 'bill_amount': 120.0}\n",
            "  Verification by severity_level passed (first and last elements).\n",
            "\n",
            "Sorting patients by 'bill_amount' (ascending):\n",
            "{'patient_id': 'PAT005', 'name': 'Eve Adams', 'severity_level': 1, 'admission_date': '2023-01-25', 'bill_amount': 120.0}\n",
            "{'patient_id': 'PAT002', 'name': 'Bob Johnson', 'severity_level': 2, 'admission_date': '2023-01-20', 'bill_amount': 300.5}\n",
            "{'patient_id': 'PAT007', 'name': 'Grace Lee', 'severity_level': 2, 'admission_date': '2023-01-30', 'bill_amount': 650.3}\n",
            "{'patient_id': 'PAT004', 'name': 'Diana Prince', 'severity_level': 3, 'admission_date': '2023-02-01', 'bill_amount': 800.2}\n",
            "{'patient_id': 'PAT001', 'name': 'Alice Smith', 'severity_level': 4, 'admission_date': '2023-01-15', 'bill_amount': 1500.75}\n",
            "{'patient_id': 'PAT006', 'name': 'Frank White', 'severity_level': 4, 'admission_date': '2023-02-05', 'bill_amount': 2100.9}\n",
            "{'patient_id': 'PAT003', 'name': 'Charlie Brown', 'severity_level': 5, 'admission_date': '2023-01-18', 'bill_amount': 3500.0}\n",
            "  Verification by bill_amount passed (first and last elements).\n",
            "\n",
            "All patient sorting demonstrations and verifications complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15a9a889"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Provide a summary of the recommended algorithms, their implementations, and how they address the specific requirements of the hospital patient record system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aa20cef"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The original task was to recommend and implement data structures and algorithms for a *retail inventory system*. However, the executed solving process focused entirely on a *hospital patient record system*. The recommendations and implementations provided are for the hospital scenario, covering:\n",
        "\n",
        "1.  **Patient search by ID:** Hash Map (Dictionary) is recommended and implemented for O(1) average time complexity, suitable for exact matches and high update frequency.\n",
        "2.  **Patient search by name:** Trie (Prefix Tree) or Full-text Search Engine is recommended for partial/fuzzy matches, while the implementation uses Binary Search on a sorted list for exact, case-sensitive matches. Binary Search offers O(log N) time complexity after an initial sort.\n",
        "3.  **Patient sorting by severity level and bill amount:** Merge Sort/Timsort is recommended and implemented, providing O(N log N) time complexity, which scales well for large datasets and frequent re-sorting.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   A detailed justification table was created, outlining recommended algorithms for a hospital patient record system based on dataset size, update frequency, and performance requirements:\n",
        "    *   **Patient Search by ID:** Hash Map (Dictionary) for O(1) average time complexity in lookup, insertion, and deletion.\n",
        "    *   **Patient Search by Name (Partial/Fuzzy Match):** Trie (Prefix Tree) for O(L) prefix searches or Full-text Search Engines for complex queries.\n",
        "    *   **Patient Sorting by Severity Level & Bill Amount:** Merge Sort/Timsort for O(N log N) time complexity, ensuring efficient and scalable sorting.\n",
        "*   A sample dataset of 7 patient records was successfully generated, each with an ID, name, severity, admission date, and bill amount.\n",
        "*   The `search_patient_by_id` function, leveraging a hash map, correctly retrieved patients by ID in O(1) average time and identified non-existent IDs. For example, `PAT001` and `PAT005` were found, while `PAT999` was not.\n",
        "*   The `search_patient_by_name` function, which uses binary search on a pre-sorted list, successfully found existing names (e.g., 'Alice Smith', 'Bob Johnson') but correctly returned `None` for non-existent names ('John Doe') and case-mismatched names ('alice smith'), demonstrating its case-sensitive nature.\n",
        "*   The `sort_patients_by_key` function, utilizing Merge Sort, successfully sorted patient data by 'severity\\_level' in descending order (e.g., highest severity 5 first, lowest severity 1 last) and by 'bill\\_amount' in ascending order (e.g., lowest bill \\$120.00 first, highest bill \\$3500.00 last). An initial `TypeError` during sorting was resolved by incorporating the original index into the sort key tuple, ensuring stable comparisons for `O(N log N)` sorting.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   Ensure consistency between the task description and the actual implementation domain. The provided solution addresses a hospital patient record system, not a retail inventory system as initially stated.\n",
        "*   Enhance the patient name search functionality to support fuzzy matching and case-insensitivity, which would be more practical for real-world user input in a hospital system.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_lEKKkzzqEnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ae9ea7c"
      },
      "source": [
        "# Task\n",
        "Recommend and implement efficient data structures and algorithms for an online food delivery application, covering order search by ID and sorting by delivery time and price, along with justifications and a summary of the approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d227fa5"
      },
      "source": [
        "## Justification Table for Algorithms (Food Delivery Scenario)\n",
        "\n",
        "### Subtask:\n",
        "Create a markdown table outlining the recommended algorithms for order search (by ID) and sorting (by delivery time and price), along with justifications for each choice based on dataset size, update frequency, and performance requirements.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c11d07d"
      },
      "source": [
        "## Justification Table for Algorithms (Food Delivery Scenario)\n",
        "\n",
        "### Subtask:\n",
        "Create a markdown table outlining the recommended algorithms for order search (by ID) and sorting (by delivery time and price), along with justifications for each choice based on dataset size, update frequency, and performance requirements.\n",
        "\n",
        "### Recommended Algorithms and Justifications\n",
        "\n",
        "| Feature              | Algorithm Recommendation           | Justification (Dataset Size, Update Frequency, Performance)                                                                                                                                                                                                                                                                                                                      | Example Use Case                                                                                                    |\n",
        "| :------------------- | :--------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------ |\n",
        "| **Order Search**     |                                    |                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                     |\n",
        "| By Order ID (Exact Match)  | Hash Map (Dictionary)              | **Dataset Size:** O(1) average time complexity for lookup, insertion, and deletion, making it highly efficient for any dataset size (small to very large) where direct access by ID is needed. <br/> **Update Frequency:** Efficient for frequent additions/deletions/updates of orders as these operations also average O(1). <br/> **Performance:** Provides near-instantaneous retrieval of specific order details, crucial for real-time order tracking and customer support. | A customer service agent quickly looking up a specific order using its unique order ID to check its status.         |\n",
        "| **Order Sorting**    |                                    |                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                     |\n",
        "| By Delivery Time     | Merge Sort / Timsort (Python's default) | **Dataset Size:** O(N log N) time complexity, which is efficient and scales well for large datasets. It's a stable sort. <br/> **Update Frequency:** Delivery times are dynamic; re-sorting is often required to reflect real-time updates. For very high frequency, consider a priority queue for 'next up' deliveries. <br/> **Performance:** Guarantees O(N log N) worst-case performance, essential for optimizing delivery routes and dispatcher views where time is critical. | A dispatcher viewing all pending orders, sorted by estimated delivery time to prioritize and assign drivers.         |\n",
        "| By Price             | Merge Sort / Timsort (Python's default) | **Dataset Size:** O(N log N) time complexity, effective for sorting financial records of varying sizes. <br/> **Update Frequency:** Order prices are generally static once placed, so re-sorting is typically batch-processed or on-demand for analysis. <br/> **Performance:** Reliable performance for sorting financial data, enabling efficient analysis of order values or for display to customers (e.g., 'sort by price'). | Generating a report of orders from highest to lowest value for financial analysis or displaying menu items by price. |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bad08c7"
      },
      "source": [
        "## Define Sample Order Data\n",
        "\n",
        "### Subtask:\n",
        "Create a sample list of order dictionaries, each containing 'order_id', 'restaurant_name', 'delivery_time', 'price', and 'order_status' attributes, to serve as the dataset for demonstrating search and sort functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7a459af"
      },
      "source": [
        "**Reasoning**:\n",
        "To fulfill the subtask, I need to create a Python list of order dictionaries as specified in the instructions. This `code_block` will define the `orders` list with sample data and then print it for verification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "48593afa",
        "outputId": "ed2f2bd2-b6ba-43cb-f397-e83375c6f710"
      },
      "source": [
        "import datetime\n",
        "\n",
        "orders = [\n",
        "    {'order_id': 'ORD001', 'restaurant_name': 'Pizza Palace', 'delivery_time': '2023-11-20 18:30', 'price': 25.99, 'order_status': 'Delivered'},\n",
        "    {'order_id': 'ORD002', 'restaurant_name': 'Burger Barn', 'delivery_time': '2023-11-20 19:00', 'price': 15.50, 'order_status': 'Pending'},\n",
        "    {'order_id': 'ORD003', 'restaurant_name': 'Sushi Spot', 'delivery_time': '2023-11-20 19:15', 'price': 45.00, 'order_status': 'In Transit'},\n",
        "    {'order_id': 'ORD004', 'restaurant_name': 'Taco Truck', 'delivery_time': '2023-11-20 18:45', 'price': 12.75, 'order_status': 'Delivered'},\n",
        "    {'order_id': 'ORD005', 'restaurant_name': 'Pasta Place', 'delivery_time': '2023-11-20 20:00', 'price': 30.20, 'order_status': 'Pending'},\n",
        "    {'order_id': 'ORD006', 'restaurant_name': 'Curry House', 'delivery_time': '2023-11-20 19:30', 'price': 22.00, 'order_status': 'Delivered'},\n",
        "    {'order_id': 'ORD007', 'restaurant_name': 'Smoothie Bar', 'delivery_time': '2023-11-20 18:00', 'price': 10.00, 'order_status': 'Cancelled'}\n",
        "]\n",
        "\n",
        "print(\"Sample order data created:\")\n",
        "for order in orders:\n",
        "    print(order)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample order data created:\n",
            "{'order_id': 'ORD001', 'restaurant_name': 'Pizza Palace', 'delivery_time': '2023-11-20 18:30', 'price': 25.99, 'order_status': 'Delivered'}\n",
            "{'order_id': 'ORD002', 'restaurant_name': 'Burger Barn', 'delivery_time': '2023-11-20 19:00', 'price': 15.5, 'order_status': 'Pending'}\n",
            "{'order_id': 'ORD003', 'restaurant_name': 'Sushi Spot', 'delivery_time': '2023-11-20 19:15', 'price': 45.0, 'order_status': 'In Transit'}\n",
            "{'order_id': 'ORD004', 'restaurant_name': 'Taco Truck', 'delivery_time': '2023-11-20 18:45', 'price': 12.75, 'order_status': 'Delivered'}\n",
            "{'order_id': 'ORD005', 'restaurant_name': 'Pasta Place', 'delivery_time': '2023-11-20 20:00', 'price': 30.2, 'order_status': 'Pending'}\n",
            "{'order_id': 'ORD006', 'restaurant_name': 'Curry House', 'delivery_time': '2023-11-20 19:30', 'price': 22.0, 'order_status': 'Delivered'}\n",
            "{'order_id': 'ORD007', 'restaurant_name': 'Smoothie Bar', 'delivery_time': '2023-11-20 18:00', 'price': 10.0, 'order_status': 'Cancelled'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd4214cd"
      },
      "source": [
        "## Implement Efficient Search by Order ID\n",
        "\n",
        "### Subtask:\n",
        "Implement a Python function that efficiently searches for an order by its unique 'order_id'. This function will use a dictionary (hash map) for O(1) average time complexity lookups, demonstrating the most efficient approach for direct ID access.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f67380b7"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires implementing the `search_order_by_id` function using a hash map and including test cases. This `code_block` will define the function, create the hash map, and execute comprehensive tests.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "7fc2672d",
        "outputId": "8ac91267-9909-4af7-ceb1-ed05ac81b334"
      },
      "source": [
        "def search_order_by_id(orders_list, order_id_to_find):\n",
        "    \"\"\"\n",
        "    Searches for an order by its unique order_id using a hash map (dictionary).\n",
        "\n",
        "    Args:\n",
        "        orders_list (list): A list of order dictionaries.\n",
        "        order_id_to_find (str): The order_id to search for.\n",
        "\n",
        "    Returns:\n",
        "        dict or None: The order dictionary if found, otherwise None.\n",
        "    \"\"\"\n",
        "    # Create a hash map (dictionary) for O(1) average time complexity lookups\n",
        "    order_hash_map = {}\n",
        "    for order in orders_list:\n",
        "        order_hash_map[order['order_id']] = order\n",
        "\n",
        "    # Look up the order_id in the hash map\n",
        "    return order_hash_map.get(order_id_to_find)\n",
        "\n",
        "# Test cases\n",
        "print(\"--- Testing search_order_by_id ---\")\n",
        "\n",
        "# Test with an existing order_id\n",
        "existing_order_id_1 = 'ORD001'\n",
        "found_order_1 = search_order_by_id(orders, existing_order_id_1)\n",
        "print(f\"Searching for Order ID '{existing_order_id_1}': {found_order_1}\")\n",
        "assert found_order_1 is not None and found_order_1['order_id'] == existing_order_id_1, f\"Test failed for existing ID {existing_order_id_1}\"\n",
        "print(f\"  Test Passed for existing ID: {existing_order_id_1}\")\n",
        "\n",
        "# Test with another existing order_id\n",
        "existing_order_id_2 = 'ORD005'\n",
        "found_order_2 = search_order_by_id(orders, existing_order_id_2)\n",
        "print(f\"Searching for Order ID '{existing_order_id_2}': {found_order_2}\")\n",
        "assert found_order_2 is not None and found_order_2['order_id'] == existing_order_id_2, f\"Test failed for existing ID {existing_order_id_2}\"\n",
        "print(f\"  Test Passed for existing ID: {existing_order_id_2}\")\n",
        "\n",
        "# Test with a non-existent order_id\n",
        "non_existent_order_id = 'ORD999'\n",
        "not_found_order = search_order_by_id(orders, non_existent_order_id)\n",
        "print(f\"Searching for Order ID '{non_existent_order_id}': {not_found_order}\")\n",
        "assert not_found_order is None, f\"Test failed for non-existent ID {non_existent_order_id}\"\n",
        "print(f\"  Test Passed for non-existent ID: {non_existent_order_id}\")\n",
        "\n",
        "print(\"All test cases passed for search_order_by_id!\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Testing search_order_by_id ---\n",
            "Searching for Order ID 'ORD001': {'order_id': 'ORD001', 'restaurant_name': 'Pizza Palace', 'delivery_time': '2023-11-20 18:30', 'price': 25.99, 'order_status': 'Delivered'}\n",
            "  Test Passed for existing ID: ORD001\n",
            "Searching for Order ID 'ORD005': {'order_id': 'ORD005', 'restaurant_name': 'Pasta Place', 'delivery_time': '2023-11-20 20:00', 'price': 30.2, 'order_status': 'Pending'}\n",
            "  Test Passed for existing ID: ORD005\n",
            "Searching for Order ID 'ORD999': None\n",
            "  Test Passed for non-existent ID: ORD999\n",
            "All test cases passed for search_order_by_id!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60c7e33c"
      },
      "source": [
        "## Demonstrate Order Sorting by Delivery Time and Price\n",
        "\n",
        "### Subtask:\n",
        "Demonstrate the usage of the existing `merge_sort` function to sort the sample order data. This will involve sorting the list of order dictionaries first by their 'delivery_time' attribute and then by their 'price' attribute, showcasing the algorithm's application.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5828593a"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires demonstrating the `merge_sort` function for sorting a list of order dictionaries by 'delivery_time' and 'price'. I will create a wrapper function `sort_orders_by_key` that transforms the list of order dictionaries into a list of `(key_value, original_index, order_dictionary)` tuples, which `merge_sort` can sort based on the `key_value` (and `original_index` for stability). After sorting, the original order dictionaries will be extracted. This approach allows using the `merge_sort` function as is, without modification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "43bc9116",
        "outputId": "7b73386b-26b0-4eb8-9aa8-6b7640f13b6a"
      },
      "source": [
        "def sort_orders_by_key(orders_list, key_name, reverse=False):\n",
        "    \"\"\"\n",
        "    Sorts a list of order dictionaries using the existing merge_sort function\n",
        "    based on a specified key.\n",
        "\n",
        "    Args:\n",
        "        orders_list (list): A list of order dictionaries.\n",
        "        key_name (str): The key in the dictionary to sort by (e.g., 'delivery_time', 'price').\n",
        "        reverse (bool): If True, sort in descending order. Defaults to False (ascending).\n",
        "\n",
        "    Returns:\n",
        "        list: A new list of order dictionaries, sorted by the specified key.\n",
        "    \"\"\"\n",
        "    # Create a temporary list of tuples (key_value, original_index, order_dictionary)\n",
        "    # The original_index ensures stable sorting and prevents direct dictionary comparison\n",
        "    # if key_values are equal, as Python will compare the indices next.\n",
        "    items_to_sort = []\n",
        "    for i, order in enumerate(orders_list):\n",
        "        items_to_sort.append((order[key_name], i, order)) # Add index 'i' here\n",
        "\n",
        "    # Use the existing merge_sort function to sort these tuples\n",
        "    # merge_sort sorts in ascending order by default (based on tuple comparison)\n",
        "    sorted_items = merge_sort(items_to_sort)\n",
        "\n",
        "    # Extract the original order dictionaries from the sorted tuples\n",
        "    sorted_orders = [item[2] for item in sorted_items] # Now extract from index 2\n",
        "\n",
        "    if reverse:\n",
        "        return sorted_orders[::-1] # Reverse the list if descending order is requested\n",
        "    return sorted_orders\n",
        "\n",
        "# Test cases for sorting\n",
        "print(\"--- Demonstrating Order Sorting by Delivery Time and Price ---\")\n",
        "\n",
        "# 1. Sort by delivery_time (ascending)\n",
        "print(\"\\nSorting orders by 'delivery_time' (ascending):\")\n",
        "sorted_by_delivery_time = sort_orders_by_key(orders, 'delivery_time', reverse=False)\n",
        "for order in sorted_by_delivery_time:\n",
        "    print(order)\n",
        "\n",
        "# Verify sorting by delivery_time\n",
        "assert sorted_by_delivery_time[0]['delivery_time'] == '2023-11-20 18:00', f\"Expected earliest delivery time '2023-11-20 18:00', got {sorted_by_delivery_time[0]['delivery_time']}\"\n",
        "assert sorted_by_delivery_time[-1]['delivery_time'] == '2023-11-20 20:00', f\"Expected latest delivery time '2023-11-20 20:00', got {sorted_by_delivery_time[-1]['delivery_time']}\"\n",
        "print(\"  Verification by delivery_time passed (first and last elements).\")\n",
        "\n",
        "# 2. Sort by price (ascending)\n",
        "print(\"\\nSorting orders by 'price' (ascending):\")\n",
        "# It's good practice to sort a copy if you need the original list untouched elsewhere\n",
        "sorted_by_price = sort_orders_by_key(orders.copy(), 'price', reverse=False)\n",
        "for order in sorted_by_price:\n",
        "    print(order)\n",
        "\n",
        "# Verify sorting by price\n",
        "assert sorted_by_price[0]['price'] == 10.00, f\"Expected lowest price 10.00, got {sorted_by_price[0]['price']}\"\n",
        "assert sorted_by_price[-1]['price'] == 45.00, f\"Expected highest price 45.00, got {sorted_by_price[-1]['price']}\"\n",
        "print(\"  Verification by price passed (first and last elements).\")\n",
        "\n",
        "print(\"\\nAll order sorting demonstrations and verifications complete!\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Demonstrating Order Sorting by Delivery Time and Price ---\n",
            "\n",
            "Sorting orders by 'delivery_time' (ascending):\n",
            "{'order_id': 'ORD007', 'restaurant_name': 'Smoothie Bar', 'delivery_time': '2023-11-20 18:00', 'price': 10.0, 'order_status': 'Cancelled'}\n",
            "{'order_id': 'ORD001', 'restaurant_name': 'Pizza Palace', 'delivery_time': '2023-11-20 18:30', 'price': 25.99, 'order_status': 'Delivered'}\n",
            "{'order_id': 'ORD004', 'restaurant_name': 'Taco Truck', 'delivery_time': '2023-11-20 18:45', 'price': 12.75, 'order_status': 'Delivered'}\n",
            "{'order_id': 'ORD002', 'restaurant_name': 'Burger Barn', 'delivery_time': '2023-11-20 19:00', 'price': 15.5, 'order_status': 'Pending'}\n",
            "{'order_id': 'ORD003', 'restaurant_name': 'Sushi Spot', 'delivery_time': '2023-11-20 19:15', 'price': 45.0, 'order_status': 'In Transit'}\n",
            "{'order_id': 'ORD006', 'restaurant_name': 'Curry House', 'delivery_time': '2023-11-20 19:30', 'price': 22.0, 'order_status': 'Delivered'}\n",
            "{'order_id': 'ORD005', 'restaurant_name': 'Pasta Place', 'delivery_time': '2023-11-20 20:00', 'price': 30.2, 'order_status': 'Pending'}\n",
            "  Verification by delivery_time passed (first and last elements).\n",
            "\n",
            "Sorting orders by 'price' (ascending):\n",
            "{'order_id': 'ORD007', 'restaurant_name': 'Smoothie Bar', 'delivery_time': '2023-11-20 18:00', 'price': 10.0, 'order_status': 'Cancelled'}\n",
            "{'order_id': 'ORD004', 'restaurant_name': 'Taco Truck', 'delivery_time': '2023-11-20 18:45', 'price': 12.75, 'order_status': 'Delivered'}\n",
            "{'order_id': 'ORD002', 'restaurant_name': 'Burger Barn', 'delivery_time': '2023-11-20 19:00', 'price': 15.5, 'order_status': 'Pending'}\n",
            "{'order_id': 'ORD006', 'restaurant_name': 'Curry House', 'delivery_time': '2023-11-20 19:30', 'price': 22.0, 'order_status': 'Delivered'}\n",
            "{'order_id': 'ORD001', 'restaurant_name': 'Pizza Palace', 'delivery_time': '2023-11-20 18:30', 'price': 25.99, 'order_status': 'Delivered'}\n",
            "{'order_id': 'ORD005', 'restaurant_name': 'Pasta Place', 'delivery_time': '2023-11-20 20:00', 'price': 30.2, 'order_status': 'Pending'}\n",
            "{'order_id': 'ORD003', 'restaurant_name': 'Sushi Spot', 'delivery_time': '2023-11-20 19:15', 'price': 45.0, 'order_status': 'In Transit'}\n",
            "  Verification by price passed (first and last elements).\n",
            "\n",
            "All order sorting demonstrations and verifications complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01d94caa"
      },
      "source": [
        "## Summary: Online Food Delivery Application\n",
        "\n",
        "The task involved recommending and implementing efficient data structures and algorithms for key operations within an online food delivery application: order search by ID, and order sorting by delivery time and price.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Order Search by ID:**\n",
        "    *   **Algorithm Recommended & Implemented:** Hash Map (Python Dictionary).\n",
        "    *   **Performance:** Achieves O(1) average time complexity for lookups. This is crucial for rapid retrieval of order details.\n",
        "    *   **Justification:** Ideal for high-volume systems where unique order IDs demand instant access for customer support, status checks, or internal logistics, regardless of dataset size or update frequency.\n",
        "\n",
        "*   **Order Sorting by Delivery Time:**\n",
        "    *   **Algorithm Recommended & Implemented:** Merge Sort / Timsort (Python's default sort).\n",
        "    *   **Performance:** Guarantees O(N log N) time complexity in all cases.\n",
        "    *   **Justification:** Essential for optimizing dispatcher views and delivery routes. As delivery times are dynamic, efficient re-sorting is often required. The stability of Merge Sort helps maintain relative order for orders with identical delivery times. For very high frequency, a priority queue could be considered for 'next up' deliveries, but for general display and batch processing, Merge Sort is robust.\n",
        "\n",
        "*   **Order Sorting by Price:**\n",
        "    *   **Algorithm Recommended & Implemented:** Merge Sort / Timsort (Python's default sort).\n",
        "    *   **Performance:** Guarantees O(N log N) time complexity in all cases.\n",
        "    *   **Justification:** Provides reliable performance for financial analysis or displaying menu items/orders to customers based on value. Order prices are typically static once placed, making periodic or on-demand sorting efficient enough for most use cases.\n",
        "\n",
        "### Implementation Details\n",
        "\n",
        "*   A `search_order_by_id` function was implemented using a Python dictionary to create a hash map, enabling O(1) average time complexity lookups for specific orders.\n",
        "*   A `sort_orders_by_key` wrapper function was created to leverage the existing `merge_sort` algorithm. This function handles dictionary-based data by transforming it into sortable tuples `(key_value, original_index, order_dictionary)`, ensuring correct sorting based on the specified key ('delivery_time' or 'price') while maintaining stability and preventing comparison errors.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The chosen algorithms provide a solid and efficient foundation for the core operations of an online food delivery system.\n",
        "*   For extremely dynamic scenarios where orders need to be constantly re-ranked based on real-time delivery updates (e.g., driver assignment queues), exploring data structures like min-heaps (priority queues) for delivery time could offer even faster retrieval of the highest priority order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a43ee362"
      },
      "source": [
        "## Summary: Online Food Delivery Application\n",
        "\n",
        "The task involved recommending and implementing efficient data structures and algorithms for key operations within an online food delivery application: order search by ID, and order sorting by delivery time and price.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Order Search by ID:**\n",
        "    *   **Algorithm Recommended & Implemented:** Hash Map (Python Dictionary).\n",
        "    *   **Performance:** Achieves O(1) average time complexity for lookups. This is crucial for rapid retrieval of order details.\n",
        "    *   **Justification:** Ideal for high-volume systems where unique order IDs demand instant access for customer support, status checks, or internal logistics, regardless of dataset size or update frequency.\n",
        "\n",
        "*   **Order Sorting by Delivery Time:**\n",
        "    *   **Algorithm Recommended & Implemented:** Merge Sort / Timsort (Python's default sort).\n",
        "    *   **Performance:** Guarantees O(N log N) time complexity in all cases.\n",
        "    *   **Justification:** Essential for optimizing dispatcher views and delivery routes. As delivery times are dynamic, efficient re-sorting is often required. The stability of Merge Sort helps maintain relative order for orders with identical delivery times. For very high frequency, a priority queue could be considered for 'next up' deliveries, but for general display and batch processing, Merge Sort is robust.\n",
        "\n",
        "*   **Order Sorting by Price:**\n",
        "    *   **Algorithm Recommended & Implemented:** Merge Sort / Timsort (Python's default sort).\n",
        "    *   **Performance:** Guarantees O(N log N) time complexity in all cases.\n",
        "    *   **Justification:** Provides reliable performance for financial analysis or displaying menu items/orders to customers based on value. Order prices are typically static once placed, making periodic or on-demand sorting efficient enough for most use cases.\n",
        "\n",
        "### Implementation Details\n",
        "\n",
        "*   A `search_order_by_id` function was implemented using a Python dictionary to create a hash map, enabling O(1) average time complexity lookups for specific orders.\n",
        "*   A `sort_orders_by_key` wrapper function was created to leverage the existing `merge_sort` algorithm. This function handles dictionary-based data by transforming it into sortable tuples `(key_value, original_index, order_dictionary)`, ensuring correct sorting based on the specified key ('delivery_time' or 'price') while maintaining stability and preventing comparison errors.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The chosen algorithms provide a solid and efficient foundation for the core operations of an online food delivery system.\n",
        "*   For extremely dynamic scenarios where orders need to be constantly re-ranked based on real-time delivery updates (e.g., driver assignment queues), exploring data structures like min-heaps (priority queues) for delivery time could offer even faster retrieval of the highest priority order.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a09062ab"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Provide a summary of the recommended algorithms, their implementations, and how they address the specific requirements of the online food delivery application.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cb69b03"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The task asked to recommend and implement efficient data structures and algorithms for an online food delivery application, covering order search by ID and sorting by delivery time and price, along with justifications.\n",
        "\n",
        "*   **Order Search by ID**: A Hash Map (implemented using a Python Dictionary) was recommended and implemented, providing O(1) average time complexity for lookups.\n",
        "*   **Order Sorting by Delivery Time**: Merge Sort / Timsort (Python's default sort) was recommended and demonstrated, offering O(N log N) time complexity.\n",
        "*   **Order Sorting by Price**: Merge Sort / Timsort (Python's default sort) was recommended and demonstrated, also offering O(N log N) time complexity.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Order Search by ID**:\n",
        "    *   **Recommended Algorithm**: Hash Map (Python Dictionary).\n",
        "    *   **Performance**: Achieves O(1) average time complexity for lookups, insertions, and deletions, ensuring near-instantaneous retrieval of specific order details.\n",
        "    *   **Implementation**: A `search_order_by_id` function was successfully implemented, utilizing a dictionary to map order IDs for efficient lookups.\n",
        "*   **Order Sorting by Delivery Time**:\n",
        "    *   **Recommended Algorithm**: Merge Sort / Timsort (Python's default sort).\n",
        "    *   **Performance**: Guarantees O(N log N) time complexity, which scales well for large datasets and is crucial for optimizing delivery routes and dispatcher views.\n",
        "    *   **Implementation**: A `sort_orders_by_key` wrapper function was created to sort a list of order dictionaries by the 'delivery\\_time' key, correctly using the existing `merge_sort` algorithm and demonstrating its effectiveness. The earliest delivery time found was '2023-11-20 18:00' and the latest was '2023-11-20 20:00'.\n",
        "*   **Order Sorting by Price**:\n",
        "    *   **Recommended Algorithm**: Merge Sort / Timsort (Python's default sort).\n",
        "    *   **Performance**: Also provides O(N log N) time complexity, suitable for financial analysis or displaying orders based on value.\n",
        "    *   **Implementation**: The `sort_orders_by_key` function was also used to sort orders by the 'price' key. The lowest price found was \\$10.00 and the highest was \\$45.00.\n",
        "*   A sample dataset of 7 order dictionaries was created, each containing `order_id`, `restaurant_name`, `delivery_time`, `price`, and `order_status` attributes, to facilitate the demonstration of these algorithms.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The chosen algorithms and data structures provide a robust and efficient foundation for critical operations within an online food delivery application, balancing performance requirements with data characteristics.\n",
        "*   For highly dynamic scenarios requiring constant re-ranking of orders based on real-time delivery updates (e.g., driver assignment queues), exploring specialized data structures like min-heaps (priority queues) for 'next up' deliveries could offer even faster retrieval of the highest priority order than general sorting.\n"
      ]
    }
  ]
}